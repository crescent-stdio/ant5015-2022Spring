{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bbdb28",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "- In this assignment, you will implement a melody-language model\n",
    "- You have to submit your code in **TWO** formats:\n",
    "    - Completed Notebook with `.ipynb`\n",
    "    - A `{your_student_number}.py` file that includes **ALL functions and classes you have completed**\n",
    "        - Do not include any other code except function and class\n",
    "        - Your result will be scored by an evaluation code that import this `{your_student_number}.py` file\n",
    "        - So be careful not to use any global variable inside the function\n",
    "- You have to submit a report (optional) and **three** generation results of your favorite in wav files\n",
    "    - The report is optional. If you have tried other architecture for MelodyLanguage Model, you can describe the result.\n",
    "\n",
    "\n",
    "- Caution: The `assert` lines are designed to check whether basic requirements are satisfied. Even though you passed all the assert cases, it doesn't guarantee that your implementation is fully correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24cd5c",
   "metadata": {},
   "source": [
    "## 0. Prepare (Install and import library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8e670ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: muspy in /opt/homebrew/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: tqdm>=4.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (4.64.0)\n",
      "Requirement already satisfied: music21>=6.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (7.3.3)\n",
      "Requirement already satisfied: joblib>=0.15 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (6.0)\n",
      "Requirement already satisfied: miditoolkit>=0.1 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.1.16)\n",
      "Requirement already satisfied: pretty-midi>=0.2 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.2.9)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (2.27.1)\n",
      "Requirement already satisfied: pypianoroll>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.0.4)\n",
      "Requirement already satisfied: bidict>=0.21 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.22.0)\n",
      "Requirement already satisfied: mido>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.2.10)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (9.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (8.13.0)\n",
      "Requirement already satisfied: jsonpickle in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (2.2.0)\n",
      "Requirement already satisfied: webcolors>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (1.12)\n",
      "Requirement already satisfied: chardet in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (4.0.0)\n",
      "Requirement already satisfied: six in /opt/homebrew/lib/python3.9/site-packages (from pretty-midi>=0.2->muspy) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/homebrew/lib/python3.9/site-packages (from pypianoroll>=1.0->muspy) (1.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (2022.5.18.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSkip downloading as the Bravura font is found.\n",
      "Skip downloading as the MuseScore General soundfont is found.\n"
     ]
    }
   ],
   "source": [
    "!pip install muspy\n",
    "import muspy\n",
    "\n",
    "muspy.download_bravura_font()\n",
    "'''\n",
    "You may have to install fluidsynth.\n",
    "In Colab, you can install by followign code\n",
    "\n",
    "!sudo apt-get install fluidsynth\n",
    "'''\n",
    "muspy.download_musescore_soundfont()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9b3ba",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding and Implementing RNN (15 pts)\n",
    "- Recurrent neural network is a typical choice for handling sequential data with a neural network\n",
    "- In this problem, you have to implement a Vanilla RNN\n",
    "    - For each time step $t$, RNN takes two inputs\n",
    "        - $x_t$, which is an input vector of time step $t$\n",
    "        - $h_{t-1}$, which is an hidden state of previous time step, $t-1$\n",
    "            - $h_{t-1}$ is also the output of RNN for previous time step $t-1$\n",
    "    - For given $x_t$ and $h_{t-1}$, RNN returns $h_t$\n",
    "        - $h_t = \\tanh(Wx_t + Uh_{t-1} + b)$ \n",
    "            - $W$ and $U$ is a trainable weight matrix of RNN\n",
    "            - $W \\in \\mathbb{R}^{d \\times h}$, and $U \\in \\mathbb{R}^{h \\times h}$, where $d$ is number of input dimension and $h$ is number of hidden state dimension\n",
    "                - This means that $W$ is a matrix with real numbers and size of $\\text{num_input_dim}\\times \\text{num_hidden_dim}$  \n",
    "                - and $U$ is a matrix with real numbers and size of $\\text{num_hidden_dim}\\times \\text{num_hidden_dim}$  \n",
    "            \n",
    " - The output of fully connected layer (`nn.Linear`) for a given input vector $x$ is as below:\n",
    "     - $\\text{output} = Wx+b$\n",
    "     - Where $W$ is a weight matrix and $b$ is a bias vector\n",
    "     - Both $W$ and $b$ are trainable parameters\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050d35e",
   "metadata": {},
   "source": [
    "### Problem 1.1: Calculating Forward Propagation of RNN\n",
    "- Based on the example above, implement the forward propagation of uni-directional, single layer vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3564e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_weight_for_hidden_to_hidden: \n",
      " tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959],\n",
      "        [ 0.5667,  0.7935,  0.4397,  0.1124,  0.6408,  0.4412],\n",
      "        [-0.2159, -0.7425,  0.5627,  0.2596,  0.5229,  2.3022],\n",
      "        [-1.4689, -1.5867,  1.2032,  0.0845, -1.2001, -0.0048]])\n",
      "example_weight_for_input_to_hidden: \n",
      " tensor([[-0.2303, -0.3918, -0.4731],\n",
      "        [ 0.3356,  1.5091,  2.0820],\n",
      "        [ 1.7067,  2.3804, -1.1256],\n",
      "        [-0.3170, -0.1407,  0.8058],\n",
      "        [ 0.3276, -0.7607, -1.5991],\n",
      "        [ 0.0185, -0.7504,  0.1854]])\n",
      "example_bias: \n",
      " tensor([-0.6776,  1.0422, -1.9513,  0.4186,  3.3214,  0.8764])\n",
      "example_input_sequence: \n",
      " tensor([[ 0.3446,  0.5199, -2.6133],\n",
      "        [-1.6965, -0.2282,  0.2800],\n",
      "        [ 0.0732,  1.1133,  0.3380],\n",
      "        [ 0.4544,  0.4569, -0.8654],\n",
      "        [ 0.7813, -0.9268,  0.2064],\n",
      "        [-0.3334, -0.0729, -0.0340],\n",
      "        [ 0.9625,  0.3492, -0.9215],\n",
      "        [-0.0562, -0.7015,  1.0367],\n",
      "        [ 1.9218, -0.4025,  0.1239],\n",
      "        [ 1.1648,  0.9234,  1.3873],\n",
      "        [ 1.3750,  0.6596, -0.8048],\n",
      "        [ 0.5656,  0.6104,  0.4669],\n",
      "        [ 1.9507, -1.0631,  1.1404],\n",
      "        [-0.0899, -0.5940, -1.2439],\n",
      "        [-0.1021, -1.0335, -0.1434],\n",
      "        [-0.3173,  0.9671, -0.9911],\n",
      "        [ 0.3016, -0.1073,  0.9985],\n",
      "        [-0.4987,  0.9910, -0.7777],\n",
      "        [ 0.3140,  0.2133, -0.1201],\n",
      "        [ 0.3605, -0.3140, -1.0787]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Don't change this cell\n",
    "'''\n",
    "example_input_size = 3\n",
    "example_hidden_size = 6\n",
    "example_sequence_length = 20\n",
    "\n",
    "torch.manual_seed(0)\n",
    "example_weight_for_hidden_to_hidden = torch.randn([example_hidden_size, example_hidden_size])\n",
    "example_weight_for_input_to_hidden = torch.randn([example_hidden_size, example_input_size])\n",
    "example_bias = torch.randn([example_hidden_size])\n",
    "example_input_sequence = torch.randn([example_sequence_length, example_input_size])\n",
    "\n",
    "print('example_weight_for_hidden_to_hidden: \\n',example_weight_for_hidden_to_hidden)\n",
    "print('example_weight_for_input_to_hidden: \\n',example_weight_for_input_to_hidden)\n",
    "print('example_bias: \\n',example_bias)\n",
    "print('example_input_sequence: \\n',example_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9752e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_single_step(current_input:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> torch.Tensor:\n",
    "  '''\n",
    "  This function \n",
    "  \n",
    "  Arguments:\n",
    "    current_input: Input vector of the current time step. Has a shape of [input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  Outputs:\n",
    "    current hidden: Updated hidden state for the current time step. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  ℎ𝑡=tanh(𝑊𝑥𝑡+𝑈ℎ𝑡−1+𝑏)\n",
    "  '''\n",
    "  return torch.tanh(ih_weight@current_input + hh_weight@prev_hidden + bias)\n",
    "\n",
    "\n",
    "def initialize_hidden_state_for_single_batch(hidden_dim:int) -> torch.Tensor:\n",
    "  '''\n",
    "  This function returns zero Tensor for a given hidden dimension. This function assumes that the RNN uses single layer and single direction.\n",
    "  \n",
    "  Argument\n",
    "    hidden_dim\n",
    "    \n",
    "  Return\n",
    "    initial_hidden_state: Has a shape of [hidden_dim]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  return torch.Tensor(hidden_dim)\n",
    "\n",
    "\n",
    "initial_hidden = initialize_hidden_state_for_single_batch(example_hidden_size)\n",
    "assert initial_hidden.shape == torch.Size([example_hidden_size])\n",
    "\n",
    "single_output = rnn_single_step(example_input_sequence[0], initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)\n",
    "assert (torch.abs(single_output - torch.Tensor([ 0.2690, -0.9982,  0.9929, -0.9535,  1.0000,  0.0081]))<1e-4).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b93b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_for_entire_timestep(input_seq:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> tuple:\n",
    "  '''\n",
    "  This function returns the output of RNN for the given 'input_seq', for the given RNN's parameters (hh_weight, ih_weight, and bias)\n",
    "  \n",
    "  Arguments:\n",
    "    input_seq: Sequence of input vector. Has a shape of [number_of_timestep, input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "\n",
    "  \n",
    "  Return: tuple (output, final_hidden_state)\n",
    "    output: Sequence of output hidden state of RNN along input timesteps. Has a a shape of [number_of_timestep, hidden_dimension]\n",
    "    final_hidden_state: Hidden state of RNN of the last time step. Has a a shape of [hidden_dimension]\n",
    "    \n",
    "  TODO: Complete this function using your 'rnn_single_step()'\n",
    "  '''\n",
    "  output = []\n",
    "  for idx in range(input_seq.shape[0]):\n",
    "    curr_input = input_seq[idx] #\n",
    "    curr_hidden = rnn_single_step(curr_input, prev_hidden, hh_weight, ih_weight, bias)\n",
    "    prev_hidden = curr_hidden\n",
    "    output.append(curr_hidden)\n",
    "    \n",
    "  final_hidden_state = curr_hidden\n",
    "  output = torch.stack(output)\n",
    "  return (output, final_hidden_state)\n",
    "\n",
    "total_output = rnn_for_entire_timestep(example_input_sequence, initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6557792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test case\n",
    "'''\n",
    "\n",
    "assert isinstance(total_output, tuple) and len(total_output)==2, \"RNN's output has to be tuple of two tensors\"\n",
    "assert isinstance(total_output[0], torch.Tensor), 'Hidden states has to be a tensor'\n",
    "assert (total_output[0][6] - torch.Tensor([ 0.8273,  0.5121, -0.5701, -0.9566,  0.9984,  0.5125])).abs().min() < 1e-4, f\"Output value is different: {total_output[0][6]}\"\n",
    "assert (total_output[1]- torch.Tensor([-0.2121, -0.9892, -0.9953,  0.7993,  1.0000, -0.9995])).abs().min() < 1e-4, f\"Output value is different: {total_output[1]}\"\n",
    "\n",
    "print(\"Passed the test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3661e",
   "metadata": {},
   "source": [
    "## Problem 2: Understanding Embedding Layer (10 pts)\n",
    "- Embedding Layer takes categorical indices and return corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc07eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[7, 6],\n",
       "          [9, 3]],\n",
       " \n",
       "         [[8, 4],\n",
       "          [4, 9]],\n",
       " \n",
       "         [[4, 4],\n",
       "          [9, 9]]]),\n",
       " tensor([[[[ 0.6430,  1.6953,  2.0655],\n",
       "           [ 0.1806,  1.3615,  2.0372]],\n",
       " \n",
       "          [[ 0.4826, -0.8298,  1.2678],\n",
       "           [-1.7174,  1.5346, -0.0032]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2578, -0.5650,  0.9278],\n",
       "           [-1.6034,  0.0581, -0.1434]],\n",
       " \n",
       "          [[-1.6034,  0.0581, -0.1434],\n",
       "           [ 0.4826, -0.8298,  1.2678]]],\n",
       " \n",
       " \n",
       "         [[[-1.6034,  0.0581, -0.1434],\n",
       "           [-1.6034,  0.0581, -0.1434]],\n",
       " \n",
       "          [[ 0.4826, -0.8298,  1.2678],\n",
       "           [ 0.4826, -0.8298,  1.2678]]]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.weight = torch.randn(num_embeddings, embedding_dim)\n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    Argument\n",
    "      x: torch.LongTensor of arbitrary shape, where each element represent categorical index smaller than self.num_embeddings\n",
    "      \n",
    "    Return\n",
    "      out (torch.Tensor): torch.FloatTensor with [shape of x, self.embedding_dim]\n",
    "    \n",
    "    TODO: Complete this function using self.weight\n",
    "    '''\n",
    "    return self.weight[x]\n",
    "  \n",
    "custom_embedding_layer = CustomEmbeddingLayer(10, example_input_size)\n",
    "random_categorical_input = torch.randint(0,10, [3, 2, 2])\n",
    "random_categorical_input, custom_embedding_layer(random_categorical_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1b7d2",
   "metadata": {},
   "source": [
    "## Problem 3: Dataset (20 pts)\n",
    "- You have to declare a path for saving dataset\n",
    "- The dataset has vocabulary information\n",
    "    - For both pitch and duration, we added `'start'` and `'end'` token\n",
    "    - This helps a language model to start the generation or end the generation\n",
    "- You have to implment `__getitem__` of this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "013d05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading source : https://ifdo.ca/~seymour/runabc/esac/esac.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178061312it [00:02, 66725741.30it/s]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded source : /Users/crescent/Code/ant5015-2022Spring/datasets/essen_folk/esac.zip .\n",
      "Extracting archive : /Users/crescent/Code/ant5015-2022Spring/datasets/essen_folk/esac.zip ...\n",
      "Successfully extracted archive : /Users/crescent/Code/ant5015-2022Spring/datasets/essen_folk .\n",
      "Converting and saving the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 10457/10457 [05:24<00:00, 32.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 9034 out of 10457 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou can download the dataset like this, but it will take too much time in Colab\\n\\nessen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\\nessen.convert()\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_path = 'datasets/essen_folk/'\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "'''\n",
    "You can download the dataset like this, but it will take too much time in Colab\n",
    "\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "'''\n",
    "# # !pip install --upgrade gdown\n",
    "# !gdown 1HMHgPifMFgRtIiLJsTb3ULqbxJx4xpQY # If it doesn't work, you have to upgrade gdown by !pip install --upgrade gdown\n",
    "\n",
    "# # Following code will automatically unzip te dataset to essen_folk/\n",
    "# !unzip -oq essen_converted.zip  # option: overwrite, quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ba4a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the `.muspy.success` file is found.\n",
      "Skip extracting as the `.muspy.success` file is found.\n",
      "Skip conversion as the `.muspy.success` file is found.\n"
     ]
    }
   ],
   "source": [
    "class MelodyDataset:\n",
    "  def __init__(self, muspy_dataset, vocabs=None):\n",
    "    self.dataset = muspy_dataset\n",
    "    \n",
    "    if vocabs is None:\n",
    "      self.idx2pitch, self.idx2dur = self._get_vocab_info()\n",
    "      self.idx2pitch += ['start', 'end']\n",
    "      self.idx2dur += ['start', 'end']\n",
    "      self.pitch2idx = {x:i for i, x in enumerate(self.idx2pitch)}\n",
    "      self.dur2idx = {x:i for i, x in enumerate(self.idx2dur)}\n",
    "      \n",
    "    else:\n",
    "      self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    \n",
    "  def _get_vocab_info(self):\n",
    "    entire_pitch = []\n",
    "    entire_dur = []\n",
    "    for note_rep in self.dataset:\n",
    "      pitch_in_piece = note_rep[:, 1]\n",
    "      dur_in_piece = note_rep[:, 2]\n",
    "      entire_pitch += pitch_in_piece.tolist()\n",
    "      entire_dur += dur_in_piece.tolist()\n",
    "    return list(set(entire_pitch)), list(set(entire_dur))\n",
    "  \n",
    "  def get_vocabs(self):\n",
    "    return self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    This dataset class returns melody information as a tensor with shape of [num_notes, 2 (pitch, duration)].\n",
    "    \n",
    "    To train a melody language model, you have to provide a sequence of original note, and a sequence of next note for given original note.\n",
    "    In other word, melody[i+1] has to be the shifted_melody[i], so that melody[i]'s next note can be retrieved by shifted_melody[i]\n",
    "    (Remember, language model is trained to predict the next upcoming word)\n",
    "    \n",
    "    Also, to make genration easier, we usually add 'start' token at the beginning of sequence, and 'end' token at the end of the sequence.\n",
    "    With these tokens, we can make the model recognize where is the start and end of the sequence explicitly.\n",
    "    \n",
    "    You have to add these tokens to the note sequence at this step.\n",
    "    \n",
    "    Argument:\n",
    "      idx (int): Index of data sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                 Has a shape of [1 (start_token) + num_notes, 2 (pitch, dur)]. \n",
    "                                 The first element of the sequence has to be the index for 'start' token for both pitch and duration.\n",
    "                                 The melody should not include 'end' token (Because we don't have to predict next note if we know that current note is 'end' token)\n",
    "      shifted_melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                         Has a shape of [num_notes + 1 (end_token), 2 (pitch, dur)]\n",
    "                                         The i'th note of shifted melody has to be the same with (i+1)'th note of melody\n",
    "                                         The shifted melody should not include 'start' token \n",
    "                                         (Because we never get a 'start' token after a note)\n",
    "                                         \n",
    "    이 데이터 세트 클래스는 [num_notes, 2(피치, 지속 시간)]의 모양을 가진 텐서로 멜로디 정보를 반환합니다.\n",
    "    멜로디 언어 모델을 교육하려면 원본 노트 시퀀스와 주어진 원본 노트에 대한 다음 노트 시퀀스를 제공해야 합니다.\n",
    "    즉, 멜로디[i+1]는 shifted_melody[i]이어야 합니다. 그래야 멜로디[i]의 다음 음을 shift_melody[i]로 검색할 수 있습니다.\n",
    "    (언어 모델은 다음 단어를 예측하도록 훈련됩니다.)\n",
    "    \n",
    "    또한 생성을 쉽게 하기 위해 보통 시퀀스의 시작 부분에 'start' 토큰을 추가하고 시퀀스 끝 부분에 'end' 토큰을 추가합니다.\n",
    "    이러한 토큰을 사용하여 모델이 시퀀스의 시작과 끝이 어디인지 명시적으로 인식하도록 할 수 있습니다.\n",
    "    \n",
    "    다음의 단계에서 이러한 토큰을 노트 시퀀스에 추가해야 합니다.\n",
    "    \n",
    "    Argument:\n",
    "      idx(int): 데이터 집합의 데이터 샘플 인덱스입니다.\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): [categorical_index_of_pitch, categorical_index_of_duration]의 순서입니다.\n",
    "                                 모양은 [1(start_token) + num_notes, 2(피치, dur)]입니다. \n",
    "                                 시퀀스의 첫 번째 요소는 피치와 지속 시간 모두에 대한 '시작' 토큰의 인덱스가 되어야 합니다.\n",
    "                                 멜로디에는 '종료' 토큰이 포함되지 않아야 합니다(현재 노트가 '종료' 토큰인 경우 다음 노트를 예측할 필요가 없기 때문입니다).\n",
    "      shifted_melody (torch.LongTensor): [categorical_index_of_pitch, categorical_index_of_duration]의 순서입니다.\n",
    "                                         모양이 [num_notes + 1(end_token), 2(피치, dur)]입니다.\n",
    "                                         이동된 멜로디의 i' 음은 (i+1)의 멜로디 음과 같아야 합니다.\n",
    "                                         이동된 멜로디에는 '시작' 토큰이 포함되지 않아야 합니다. \n",
    "                                         (메모 뒤에 '시작' 토큰이 나타나지 않기 때문입니다.)\n",
    "    TODO: Complete this function\n",
    "       now pitch dur  ?\n",
    "    [   0   72   24   64]\n",
    "    '''\n",
    "    note_representation = self.dataset[idx]\n",
    "    note_representation = note_representation[:,1:3].tolist()\n",
    "    melody_representation = [['start', 'start']] + note_representation\n",
    "    shifted_melody_representation = note_representation + [['end', 'end']]\n",
    "    \n",
    "    melody = []\n",
    "    for pitch_dur in melody_representation:\n",
    "      melody.append([self.pitch2idx[pitch_dur[0]], self.dur2idx[pitch_dur[1]]])\n",
    "  \n",
    "    shifted_melody = []\n",
    "    for pitch_dur in shifted_melody_representation:\n",
    "      shifted_melody.append([self.pitch2idx[pitch_dur[0]], self.dur2idx[pitch_dur[1]]])\n",
    "\n",
    "    return torch.LongTensor(melody), torch.LongTensor(shifted_melody)\n",
    "#     return torch.cat(melody, shifted_melody, dim=0)\n",
    "\n",
    "your_path = 'datasets/essen_folk/'\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "\n",
    "essen_entire = essen.to_pytorch_dataset(representation='note')\n",
    "essen_split = essen.to_pytorch_dataset(representation='note', splits=(0.8, 0.1, 0.1), random_state=0)\n",
    "entire_set = MelodyDataset(essen_entire)\n",
    "\n",
    "train_set = MelodyDataset(essen_split['train'], vocabs=entire_set.get_vocabs())\n",
    "valid_set = MelodyDataset(essen_split['validation'], vocabs=entire_set.get_vocabs())\n",
    "test_set = MelodyDataset(essen_split['test'], vocabs=entire_set.get_vocabs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0372bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To check the MelodyDataset implementation\n",
    "'''\n",
    "\n",
    "assert len(train_set[0]) == 2, \"You have to return two variables at __getitem__\"\n",
    "assert train_set[0][0].shape == train_set[0][1].shape, \"Shape of Melody and Shifted melody has to be the same\"\n",
    "\n",
    "assert (train_set[0][0][0] == torch.LongTensor([38, 44])).all(), \"You have to add start token at the beginning of melody\"\n",
    "assert (train_set[0][1][-1] == torch.LongTensor([39, 45])).all(), \"You have to add end token at the end of melody\"\n",
    "\n",
    "assert (train_set[0][0][-1] == torch.LongTensor([12, 26])).all(), \"Last part of melody must not include the end token\"\n",
    "assert (train_set[0][1][0] == torch.LongTensor([24, 16])).all(),  \"First part of shifted melody must not include the start token\"\n",
    "\n",
    "assert (train_set[20][0][1:] == train_set[20][1][:-1]).all(), \"Check the melody shift\"\n",
    "\n",
    "print(\"Passed test cases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673c342",
   "metadata": {},
   "source": [
    "## PackSequence\n",
    "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
    "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
    "- In this problem, you will learn about how to handle sequences of different length as a batch\n",
    "\n",
    "- You can also refer [a video lecture](https://youtu.be/IQf1zu6jdCU) in Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83754613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [58, 2] at entry 0 and [25, 2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis cell will make error, because the length of each sample is different to each other\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:618\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    620\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [58, 2] at entry 0 and [25, 2] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "This cell will make error, because the length of each sample is different to each other\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58708cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To handle that problem, you have to make your collate function \n",
    "'''\n",
    "def your_collate_function(raw_batch):\n",
    "  '''\n",
    "  You can make your own function to handle the batch\n",
    "  '''\n",
    "  ret_batch = raw_batch[:-1]\n",
    "  ret_batch.append(raw_batch[0])\n",
    "#   return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
    "  return ret_batch\n",
    "\n",
    "batch_size = 8\n",
    "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
    "batch = your_collate_function(raw_batch)\n",
    "\n",
    "'''\n",
    "This is what the 'collate_fn' does in DataLoader\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
    "batch_by_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a209",
   "metadata": {},
   "source": [
    "#### Pad Sequence and Pack Sequence\n",
    "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
    "- `torch.nn.utils.rnn.pad_sequence`\n",
    "    - This function takes list of tensors with different length and return padded sequence\n",
    "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
    "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
    "    - In default, we use 0 for padding (zero padding)\n",
    "    - The result \n",
    "- `torch.nn.utils.rnn.pack_sequence`\n",
    "    - pad_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2a21e",
   "metadata": {},
   "source": [
    "Below cells show the example of pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24424475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  2.],\n",
       "        [ 1.,  6.,  3.],\n",
       "        [ 2.,  8.,  4.],\n",
       "        [ 0., 12.,  3.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  2.,  0.],\n",
       "        [ 0.,  3.,  0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
    "short = torch.Tensor([0, 1, 2])\n",
    "long = torch.Tensor([3, 6, 8, 12, 1, 2, 3])\n",
    "middle = torch.Tensor([2, 3, 4, 3, 0])\n",
    "\n",
    "pad_sequence([short, long, middle], batch_first=False)  # T x N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99aec137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.,  0.,  0.,  0.],\n",
       "        [ 3.,  6.,  8., 12.,  1.,  2.,  3.],\n",
       "        [ 2.,  3.,  4.,  3.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default value of batch_first in pad_sequence is False.\n",
    "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
    "pad_sequence([short, long, middle], batch_first=True)  # N x T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b420b0",
   "metadata": {},
   "source": [
    "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
    "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
    "\n",
    "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
    "- RNN doesn't know whether it is padded data, or existing data\n",
    "- This makes computation slower\n",
    "\n",
    "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
    "\n",
    "To solve this issue, we use PackedSequence, by using `pack_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f553adab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 3.,  2.,  0.,  6.,  3.,  1.,  8.,  4.,  2., 12.,  3.,  1.,  0.,  2.,\n",
       "         3.]), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
    "packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c04cfe",
   "metadata": {},
   "source": [
    "`PackedSequence` has `data` and `batch_sizes`\n",
    "- `data` contains the flattened value of given batch\n",
    "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
    "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
    "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
    "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
    "    - `[1,2,0]` means that \n",
    "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
    "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
    "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
    "- `unsorted_indices` shows how the original sequences are sorted.\n",
    "    - `[2,0,1]` means that\n",
    "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc405b5",
   "metadata": {},
   "source": [
    "If you feed PackedSequence to RNN (or LSTM, GRU), it will return PackedSequence with same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a65589f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of output of RNN for PackedSequence: <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "Type of last_hidden of RNN for PackedSequence: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "rnn_layer = nn.GRU(1, 1)\n",
    "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
    "out, last_hidden = rnn_layer(packed_sequence)\n",
    "\n",
    "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
    "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df707e",
   "metadata": {},
   "source": [
    "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
    "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
    "- There are two ways to feed `PackedSequence` to these layers\n",
    "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
    "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e983f58",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not PackedSequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis will make error, because other layers cannot handle PackedSequence\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_linear_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_linear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_sequence\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not PackedSequence"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This will make error, because other layers cannot handle PackedSequence\n",
    "'''\n",
    "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
    "test_linear_layer(packed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1754e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded sequence generated from packed sequence (squeezed for printing): \n",
      " tensor([[ 0.,  3.,  2.],\n",
      "        [ 1.,  6.,  3.],\n",
      "        [ 2.,  8.,  4.],\n",
      "        [ 0., 12.,  3.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  2.,  0.],\n",
      "        [ 0.,  3.,  0.]])\n",
      "\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \n",
      " tensor([3, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
    "'''\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
    "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
    "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7871b5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of feeding padded_sequence to a linear layer: tensor([[[ 0.3958, -0.1259],\n",
      "         [ 0.8953,  1.1519],\n",
      "         [ 0.7288,  0.7260]],\n",
      "\n",
      "        [[ 0.5623,  0.3001],\n",
      "         [ 1.3948,  2.4297],\n",
      "         [ 0.8953,  1.1519]],\n",
      "\n",
      "        [[ 0.7288,  0.7260],\n",
      "         [ 1.7278,  3.2816],\n",
      "         [ 1.0618,  1.5778]],\n",
      "\n",
      "        [[ 0.3958, -0.1259],\n",
      "         [ 2.3938,  4.9853],\n",
      "         [ 0.8953,  1.1519]],\n",
      "\n",
      "        [[ 0.3958, -0.1259],\n",
      "         [ 0.5623,  0.3001],\n",
      "         [ 0.3958, -0.1259]],\n",
      "\n",
      "        [[ 0.3958, -0.1259],\n",
      "         [ 0.7288,  0.7260],\n",
      "         [ 0.3958, -0.1259]],\n",
      "\n",
      "        [[ 0.3958, -0.1259],\n",
      "         [ 0.8953,  1.1519],\n",
      "         [ 0.3958, -0.1259]]], grad_fn=<ViewBackward0>)\n",
      "Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now you can feed padded sequence to linear layer.\n",
    "'''\n",
    "\n",
    "linear_output = test_linear_layer(padded_sequence)\n",
    "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
    "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71d104d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.8953,  1.1519],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 0.3958, -0.1259],\n",
       "        [ 1.3948,  2.4297],\n",
       "        [ 0.8953,  1.1519],\n",
       "        [ 0.5623,  0.3001],\n",
       "        [ 1.7278,  3.2816],\n",
       "        [ 1.0618,  1.5778],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 2.3938,  4.9853],\n",
       "        [ 0.8953,  1.1519],\n",
       "        [ 0.5623,  0.3001],\n",
       "        [ 0.3958, -0.1259],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 0.8953,  1.1519]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
    "'''\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
    "re_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a869f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.8953,  1.1519],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 0.3958, -0.1259],\n",
       "        [ 1.3948,  2.4297],\n",
       "        [ 0.8953,  1.1519],\n",
       "        [ 0.5623,  0.3001],\n",
       "        [ 1.7278,  3.2816],\n",
       "        [ 1.0618,  1.5778],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 2.3938,  4.9853],\n",
       "        [ 0.8953,  1.1519],\n",
       "        [ 0.5623,  0.3001],\n",
       "        [ 0.3958, -0.1259],\n",
       "        [ 0.7288,  0.7260],\n",
       "        [ 0.8953,  1.1519]], grad_fn=<AddmmBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Another way to do it is using PackedSequence.data\n",
    "'''\n",
    "\n",
    "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
    "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
    "packed_sequence_after_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef97f8e",
   "metadata": {},
   "source": [
    "## Problem 4: Implement pack_collate(), (20 pts)\n",
    "- Implement a collate function that returns PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "518ed3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([341, 2]) torch.Size([341, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [19, 10],\n",
       "         [24, 16],\n",
       "         [27, 14],\n",
       "         [24, 10],\n",
       "         [34, 10],\n",
       "         [24, 10],\n",
       "         [26, 16],\n",
       "         [28, 16],\n",
       "         [17,  5],\n",
       "         [24, 16],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [31, 10],\n",
       "         [26, 22],\n",
       "         [21, 16],\n",
       "         [16, 16],\n",
       "         [16,  5],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [26, 10],\n",
       "         [29, 16],\n",
       "         [31, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [20, 10],\n",
       "         [29, 10],\n",
       "         [34, 10],\n",
       "         [26, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [14,  5],\n",
       "         [22, 10],\n",
       "         [22, 10],\n",
       "         [24, 26],\n",
       "         [24, 22],\n",
       "         [28,  5],\n",
       "         [21, 16],\n",
       "         [21,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [15, 10],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [26,  5],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [17, 16],\n",
       "         [26,  5],\n",
       "         [24, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [27, 14],\n",
       "         [24,  5],\n",
       "         [29, 10],\n",
       "         [21, 10],\n",
       "         [26, 16],\n",
       "         [26, 10],\n",
       "         [28,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [34,  5],\n",
       "         [24, 10],\n",
       "         [19, 10],\n",
       "         [28, 10],\n",
       "         [26, 33],\n",
       "         [19, 26],\n",
       "         [22, 10],\n",
       "         [19, 10],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [23, 26],\n",
       "         [24, 14],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [17, 26],\n",
       "         [29, 22],\n",
       "         [19, 10],\n",
       "         [14, 16],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [14, 26],\n",
       "         [26, 10],\n",
       "         [16, 16],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [15, 10],\n",
       "         [17, 16],\n",
       "         [34, 10],\n",
       "         [24, 16],\n",
       "         [31, 10],\n",
       "         [19,  5],\n",
       "         [19,  5],\n",
       "         [22, 10],\n",
       "         [17, 16],\n",
       "         [17, 10],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [27, 14],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [28, 10],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [14, 22],\n",
       "         [24, 10],\n",
       "         [29,  5],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [26,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [15, 10],\n",
       "         [27, 10],\n",
       "         [24, 10],\n",
       "         [34, 14],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23, 10],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 26],\n",
       "         [27, 10],\n",
       "         [17,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [21, 10],\n",
       "         [19,  5],\n",
       "         [19, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [19,  5],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [16, 10],\n",
       "         [23,  5],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [22, 16],\n",
       "         [24, 10],\n",
       "         [34,  5],\n",
       "         [21, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [26, 10],\n",
       "         [29, 14],\n",
       "         [20, 10],\n",
       "         [21,  5],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [19,  5],\n",
       "         [24, 10],\n",
       "         [26,  5],\n",
       "         [20, 16],\n",
       "         [19,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [14, 26],\n",
       "         [16, 26],\n",
       "         [21, 10],\n",
       "         [24, 10],\n",
       "         [17, 10],\n",
       "         [17, 26],\n",
       "         [31,  5],\n",
       "         [14, 26],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [29, 22],\n",
       "         [19,  5],\n",
       "         [26, 10],\n",
       "         [12, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [29, 10],\n",
       "         [10, 16],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [24, 16],\n",
       "         [27, 10],\n",
       "         [21, 10],\n",
       "         [14, 26],\n",
       "         [22, 10],\n",
       "         [27,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [24, 26],\n",
       "         [22, 10],\n",
       "         [21,  5],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [27, 10],\n",
       "         [19,  5],\n",
       "         [26,  5],\n",
       "         [19,  5],\n",
       "         [22, 16],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19,  5],\n",
       "         [20, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [15, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [14, 10],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [12, 10],\n",
       "         [14,  5],\n",
       "         [14, 10],\n",
       "         [14, 10],\n",
       "         [10, 26],\n",
       "         [12, 16],\n",
       "         [26, 10],\n",
       "         [12, 26],\n",
       "         [26, 10],\n",
       "         [17, 10],\n",
       "         [31, 10],\n",
       "         [15, 16],\n",
       "         [26,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19, 26],\n",
       "         [21, 26],\n",
       "         [24, 26],\n",
       "         [26, 10],\n",
       "         [19, 16],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [26,  5],\n",
       "         [17, 22],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [17, 10],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [19,  5],\n",
       "         [14, 10],\n",
       "         [17, 10],\n",
       "         [12, 26],\n",
       "         [14, 10],\n",
       "         [19, 10],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17,  5],\n",
       "         [14, 25],\n",
       "         [19, 10],\n",
       "         [17,  5],\n",
       "         [16,  5],\n",
       "         [14, 10],\n",
       "         [14,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [28,  5],\n",
       "         [26, 33],\n",
       "         [24, 14],\n",
       "         [21,  5],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [14, 22],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 10],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [14, 26]]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "         8, 6, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[19, 10],\n",
       "         [24, 16],\n",
       "         [27, 14],\n",
       "         [24, 10],\n",
       "         [34, 10],\n",
       "         [24, 10],\n",
       "         [26, 16],\n",
       "         [28, 16],\n",
       "         [17,  5],\n",
       "         [24, 16],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [31, 10],\n",
       "         [26, 22],\n",
       "         [21, 16],\n",
       "         [16, 16],\n",
       "         [16,  5],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [26, 10],\n",
       "         [29, 16],\n",
       "         [31, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [20, 10],\n",
       "         [29, 10],\n",
       "         [34, 10],\n",
       "         [26, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [14,  5],\n",
       "         [22, 10],\n",
       "         [22, 10],\n",
       "         [24, 26],\n",
       "         [24, 22],\n",
       "         [28,  5],\n",
       "         [21, 16],\n",
       "         [21,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [15, 10],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [26,  5],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [17, 16],\n",
       "         [26,  5],\n",
       "         [24, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [27, 14],\n",
       "         [24,  5],\n",
       "         [29, 10],\n",
       "         [21, 10],\n",
       "         [26, 16],\n",
       "         [26, 10],\n",
       "         [28,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [34,  5],\n",
       "         [24, 10],\n",
       "         [19, 10],\n",
       "         [28, 10],\n",
       "         [26, 33],\n",
       "         [19, 26],\n",
       "         [22, 10],\n",
       "         [19, 10],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [23, 26],\n",
       "         [24, 14],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [17, 26],\n",
       "         [29, 22],\n",
       "         [19, 10],\n",
       "         [14, 16],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [14, 26],\n",
       "         [26, 10],\n",
       "         [16, 16],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [15, 10],\n",
       "         [17, 16],\n",
       "         [34, 10],\n",
       "         [24, 16],\n",
       "         [31, 10],\n",
       "         [19,  5],\n",
       "         [19,  5],\n",
       "         [22, 10],\n",
       "         [17, 16],\n",
       "         [17, 10],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [27, 14],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [28, 10],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [14, 22],\n",
       "         [24, 10],\n",
       "         [29,  5],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [26,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [15, 10],\n",
       "         [27, 10],\n",
       "         [24, 10],\n",
       "         [34, 14],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23, 10],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 26],\n",
       "         [27, 10],\n",
       "         [17,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [21, 10],\n",
       "         [19,  5],\n",
       "         [19, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [19,  5],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [16, 10],\n",
       "         [23,  5],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [22, 16],\n",
       "         [24, 10],\n",
       "         [34,  5],\n",
       "         [21, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [26, 10],\n",
       "         [29, 14],\n",
       "         [20, 10],\n",
       "         [21,  5],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [19,  5],\n",
       "         [24, 10],\n",
       "         [26,  5],\n",
       "         [20, 16],\n",
       "         [19,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [14, 26],\n",
       "         [16, 26],\n",
       "         [21, 10],\n",
       "         [24, 10],\n",
       "         [17, 10],\n",
       "         [17, 26],\n",
       "         [31,  5],\n",
       "         [14, 26],\n",
       "         [39, 45],\n",
       "         [39, 45],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [29, 22],\n",
       "         [39, 45],\n",
       "         [19,  5],\n",
       "         [26, 10],\n",
       "         [12, 10],\n",
       "         [19,  5],\n",
       "         [39, 45],\n",
       "         [21,  5],\n",
       "         [29, 10],\n",
       "         [10, 16],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [24, 16],\n",
       "         [27, 10],\n",
       "         [21, 10],\n",
       "         [14, 26],\n",
       "         [22, 10],\n",
       "         [27,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [24, 26],\n",
       "         [22, 10],\n",
       "         [21,  5],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [27, 10],\n",
       "         [19,  5],\n",
       "         [26,  5],\n",
       "         [19,  5],\n",
       "         [22, 16],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19,  5],\n",
       "         [20, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [15, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [14, 10],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [12, 10],\n",
       "         [14,  5],\n",
       "         [14, 10],\n",
       "         [14, 10],\n",
       "         [10, 26],\n",
       "         [12, 16],\n",
       "         [26, 10],\n",
       "         [12, 26],\n",
       "         [39, 45],\n",
       "         [39, 45],\n",
       "         [26, 10],\n",
       "         [17, 10],\n",
       "         [31, 10],\n",
       "         [15, 16],\n",
       "         [26,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19, 26],\n",
       "         [21, 26],\n",
       "         [24, 26],\n",
       "         [26, 10],\n",
       "         [19, 16],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [26,  5],\n",
       "         [17, 22],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [17, 10],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [19,  5],\n",
       "         [14, 10],\n",
       "         [17, 10],\n",
       "         [12, 26],\n",
       "         [14, 10],\n",
       "         [39, 45],\n",
       "         [19, 10],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17,  5],\n",
       "         [14, 25],\n",
       "         [19, 10],\n",
       "         [17,  5],\n",
       "         [16,  5],\n",
       "         [14, 10],\n",
       "         [14,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [28,  5],\n",
       "         [26, 33],\n",
       "         [24, 14],\n",
       "         [21,  5],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [14, 22],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 10],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [14, 26],\n",
       "         [39, 45]]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "         8, 6, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "def pack_collate(raw_batch:list):\n",
    "  '''\n",
    "  This function takes a list of data, and returns two PackedSequences\n",
    "  \n",
    "  Argument\n",
    "    raw_batch: A list of MelodyDataset[idx]. Each item in the list is a tuple of (melody, shifted_melody)\n",
    "               melody and shifted_melody has a shape of [num_notes (+1 if you don't consider \"start\" and \"end\" token as note), 2]\n",
    "  Returns\n",
    "    packed_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "    packed_shifted_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "\n",
    "  TODO: Complete this function\n",
    "  '''  \n",
    "  melody = [x[0] for x in raw_batch]\n",
    "  shifted_melody = [x[1] for x in raw_batch]\n",
    "  \n",
    "  melody.sort(key=lambda x:len(x), reverse=True)\n",
    "  shifted_melody.sort(key=lambda x:len(x), reverse=True)\n",
    "  \n",
    "  packed_melody = pack_sequence(melody)\n",
    "  packed_shifted_melody = pack_sequence(shifted_melody)\n",
    "  return packed_melody, packed_shifted_melody\n",
    "\n",
    "raw_batch = [train_set[i] for i in range(batch_size)]\n",
    "packed_melody, packed_shifted_melody = pack_collate(raw_batch)\n",
    "print(packed_melody.data.shape, packed_shifted_melody.data.shape)\n",
    "packed_melody, packed_shifted_melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff1e46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test whether you have implemented pack_collate correctly\n",
    "'''\n",
    "\n",
    "assert isinstance(packed_melody, PackedSequence)\n",
    "assert isinstance(packed_shifted_melody, PackedSequence)\n",
    "\n",
    "assert packed_melody.data.shape==packed_shifted_melody.data.shape\n",
    "\n",
    "print(\"Passed all the test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d182eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         ...,\n",
       "         [21, 26],\n",
       "         [19, 38],\n",
       "         [17, 30]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 62, 62, 62, 62, 62, 62, 62, 62, 60, 57, 57, 55, 54, 54, 51, 50, 49,\n",
       "         48, 45, 45, 44, 44, 42, 42, 41, 40, 39, 37, 35, 34, 33, 32, 29, 28, 27,\n",
       "         26, 26, 23, 20, 20, 19, 19, 19, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 11, 11, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  8,  8,  8,  8,  7,  7,  7,  7,  6,  6,  5,  4,  4,  4,  4,  4,\n",
       "          4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[17, 38],\n",
       "         [18,  5],\n",
       "         [21, 10],\n",
       "         ...,\n",
       "         [19, 38],\n",
       "         [17, 30],\n",
       "         [39, 45]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 62, 62, 62, 62, 62, 62, 62, 62, 60, 57, 57, 55, 54, 54, 51, 50, 49,\n",
       "         48, 45, 45, 44, 44, 42, 42, 41, 40, 39, 37, 35, 34, 33, 32, 29, 28, 27,\n",
       "         26, 26, 23, 20, 20, 19, 19, 19, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 11, 11, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  8,  8,  8,  8,  7,  7,  7,  7,  6,  6,  5,  4,  4,  4,  4,  4,\n",
       "          4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263352",
   "metadata": {},
   "source": [
    "## Problem 5: Define Melody Language Model (25 pts)\n",
    "- In this problem, you have to define a Language Model for model\n",
    "    - It is almost same as an ordinary language model for natural language processing\n",
    "    - The key difference is that the melody language model has to predict pitch **and** duration\n",
    "- Complete the model step-by-step\n",
    "    - Complete each function and test the function with the cells below\n",
    "    - `get_concat_embedding()` makes concatenated embedding for each note given pitch and duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "893efba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[0.0278, 0.0266, 0.0262,  ..., 0.0229, 0.0220, 0.0250],\n",
       "         [0.0278, 0.0266, 0.0262,  ..., 0.0229, 0.0220, 0.0250],\n",
       "         [0.0278, 0.0266, 0.0262,  ..., 0.0229, 0.0220, 0.0250],\n",
       "         ...,\n",
       "         [0.0255, 0.0280, 0.0270,  ..., 0.0217, 0.0212, 0.0270],\n",
       "         [0.0260, 0.0275, 0.0265,  ..., 0.0218, 0.0217, 0.0269],\n",
       "         [0.0263, 0.0269, 0.0264,  ..., 0.0220, 0.0223, 0.0266]],\n",
       "        grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 62, 62, 62, 62, 62, 62, 62, 62, 60, 57, 57, 55, 54, 54, 51, 50, 49,\n",
       "         48, 45, 45, 44, 44, 42, 42, 41, 40, 39, 37, 35, 34, 33, 32, 29, 28, 27,\n",
       "         26, 26, 23, 20, 20, 19, 19, 19, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 11, 11, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  8,  8,  8,  8,  7,  7,  7,  7,  6,  6,  5,  4,  4,  4,  4,  4,\n",
       "          4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[0.0249, 0.0215, 0.0250,  ..., 0.0237, 0.0246, 0.0213],\n",
       "         [0.0249, 0.0215, 0.0250,  ..., 0.0237, 0.0246, 0.0213],\n",
       "         [0.0249, 0.0215, 0.0250,  ..., 0.0237, 0.0246, 0.0213],\n",
       "         ...,\n",
       "         [0.0258, 0.0216, 0.0249,  ..., 0.0233, 0.0231, 0.0226],\n",
       "         [0.0257, 0.0216, 0.0247,  ..., 0.0234, 0.0232, 0.0225],\n",
       "         [0.0259, 0.0215, 0.0245,  ..., 0.0237, 0.0234, 0.0225]],\n",
       "        grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 62, 62, 62, 62, 62, 62, 62, 62, 60, 57, 57, 55, 54, 54, 51, 50, 49,\n",
       "         48, 45, 45, 44, 44, 42, 42, 41, 40, 39, 37, 35, 34, 33, 32, 29, 28, 27,\n",
       "         26, 26, 23, 20, 20, 19, 19, 19, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 11, 11, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  8,  8,  8,  8,  7,  7,  7,  7,  6,  6,  5,  4,  4,  4,  4,  4,\n",
       "          4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "class MelodyLanguageModel(nn.Module):\n",
    "  def __init__(self, hidden_size, embed_size, vocabs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embed_size = embed_size\n",
    "    self.num_pitch = len(self.idx2pitch)\n",
    "    self.num_dur = len(self.idx2dur)\n",
    "    self.num_layers = 3\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    TODO: Declare four modules. Please follow the name strictly.\n",
    "      1) self.pitch_embedder: nn.Embedding layer that embed pitch category index to a vector with size of 'embed_size'\n",
    "      2) self.dur_embedder = nn.Embedding layer that embed duration category index to a vector with size of 'embed_size'\n",
    "      3) self.rnn = nn.GRU layer that takes concatenated_embedding \n",
    "                    and has a hidden size of 'hidden_size', num_layers of self.num_layers, and batch_first=True\n",
    "      4) self.final_layer = nn.Linear layer that takes self.rnn's output and convert it \n",
    "                            to logits (that can be used as input of softmax) of pitch + duration\n",
    "   \n",
    "   TODO: 모듈 4개를 선언합니다. 이름을 엄격하게 지켜주세요.\n",
    "      1) self.pitch_embedder: 'embed_size' 크기의 벡터에 피치 범주 인덱스를 포함하는 nn.Embedding layer.\n",
    "      2) self.dur_embedder = duration category index를 'embed_size' 크기의 벡터에 포함하는 nn.Embedding layer.\n",
    "      3) self.rnn = concatated_size를 사용하고 숨겨진 크기가 'hidden_size'인 GRU 계층, num_layers of self.num_layers,  batch_first=True\n",
    "      4) self.final_layer = [nn.Linear layer] self.rnn의 출력을 가져와서 \n",
    "                            'pitch + duration'의 로짓(소프트맥스의 입력으로 사용될 수 있음)으로 변환\n",
    "      self.rnn = GRU(80, 64, num_layers=3, batch_first=True)\n",
    "    '''    \n",
    "    \n",
    "    self.pitch_embedder = nn.Embedding(self.num_pitch, self.embed_size)\n",
    "    self.dur_embedder = nn.Embedding(self.num_dur, self.embed_size)\n",
    "    self.rnn =  nn.GRU(2*self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "    self.final_layer = nn.Linear(self.hidden_size, self.num_pitch + self.num_dur)\n",
    "    \n",
    "  def get_concat_embedding(self, input_seq):\n",
    "    '''\n",
    "    This function returns concatenated pitch embedding and duration embedding for a given input seq\n",
    "    \n",
    "    Arguments:\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or [num_timesteps (num_notes), 2]\n",
    "                 \n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 [타임스텝의 수 (num_notes), 2]\n",
    "    Return:\n",
    "      concat_embedding: A batch of sequence of concatenated embedding of pitch embedding and duration embedding.\n",
    "                        Has a shape of [num_batch, num_timesteps (num_notes), embedding_size * 2]\n",
    "                        Each vector of time t is [pitch_embedding ; duration_embedding] (concatenation)\n",
    "                        \n",
    "                        pitch embedding is the output of an nn.Embedding layer of given note pitch index\n",
    "                        duration embedding is the output of an nn.Embedding layer of given note duration index\n",
    "    \n",
    "    \n",
    "    TODO: Complete this function using self.pitch_embedder and self.dur_embedder\n",
    "    You can use torch.cat to concatenate two tensors or vectors\n",
    "    '''\n",
    "#     print(input_seq)\n",
    "    input_pitch = input_seq[..., 0]\n",
    "    input_dur = input_seq[..., 1]\n",
    "    return torch.cat([self.pitch_embedder(input_pitch), self.dur_embedder(input_dur)], dim=-1)\n",
    "  \n",
    "  \n",
    "  def initialize_rnn(self, batch_size: int) -> torch.Tensor :\n",
    "    '''\n",
    "    This function returns initial hidden state for self.rnn for given batch_size\n",
    "    \n",
    "    Argument\n",
    "      batch_size (int): \n",
    "      \n",
    "    Return\n",
    "      initial_hidden_state (torch.Tensor):\n",
    "    '''\n",
    "    \n",
    "    return torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "  \n",
    "    \n",
    "  \n",
    "  def forward(self, input_seq:torch.LongTensor):\n",
    "    '''\n",
    "    Forward propgation of Melody Language Model.\n",
    "    \n",
    "    Argument\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or can be a PackedSequence\n",
    "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치. \n",
    "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 PackedSequence.\n",
    "    \n",
    "    Output\n",
    "      pitch_dist: Probability distribution of pitch of next upcoming note for each timestep 't'.\n",
    "                  Has a shape of [num_batch, numtimesteps, self.num_pitch]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 음고의 확률 분포\n",
    "      dur_dist: Probability distribution of duration of next upcoming note for each timestep 't'.\n",
    "                Has a shape of [num_batch, numtimesteps, self.num_dur]\n",
    "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 길이의 확률 분포\n",
    "      \n",
    "    '''\n",
    "      \n",
    "  \n",
    "    '''\n",
    "    TODO: Complete this function. You have to handle both cases: input_seq as ordinary Tensor / input_seq as PackedSequence\n",
    "    If the input_seq is PackedSequence, return PackedSequence\n",
    "    \n",
    "    \n",
    "    input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration]\n",
    "    \n",
    "    Follow the instruction\n",
    "    '''\n",
    "\n",
    "    if isinstance(input_seq, torch.Tensor): # If input is an ordinary tensor\n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      x = self.get_concat_embedding(input_seq)\n",
    "      \n",
    "      # 2. Put concatenated_embeddings to self.rnn.\n",
    "      # Remember: RNN, GRU, LSTM returns two outputs\n",
    "      h_0 = self.initialize_rnn(batch_size=x.size(0))\n",
    "      out, _ = self.rnn(x, h_0)\n",
    "      \n",
    "      # 3. Put rnn's output with a shape of [num_batch, num_timestep, hidden_size] to self.final_layer\n",
    "      logits = self.final_layer(out)\n",
    "      \n",
    "      # 4. Convert logits (output of self.final_layer) to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      # 4. 로짓(self.final_layer 출력)을 피치 확률 및 지속 확률로 변환합니다.\n",
    "      # 조심하세요! 소프트맥스 피치와 지속시간을 따로 구하셔야 합니다.\n",
    "      # 왜냐하면 확률 분포에서 피치 1개와 지속 시간 1개를 선택해야 하기 때문입니다.\n",
    "      \n",
    "      pitch_out = logits[..., :self.num_pitch]\n",
    "      dur_out = logits[..., self.num_pitch:]\n",
    "      pitch_out = torch.softmax(pitch_out, dim=-1)\n",
    "      dur_out = torch.softmax(dur_out, dim=-1)\n",
    "\n",
    "      return pitch_out, dur_out\n",
    "    \n",
    "    elif isinstance(input_seq, PackedSequence):      \n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      # To get concatenated_embeddings, You have to either pad_packed_sequence(input_seq, batch_first=True)\n",
    "      # Or use input_seq.data, \n",
    "      # and then make new PackedSequence using concatenated_embeddings as data, and copy batch_lengths, sorted_indices, unsorted_indices.\n",
    "      '''\n",
    "      1) pack -> pad, using pad_packed_sequence(input_seq, batch_first=True)\n",
    "      2) 새로운 PackedSequence(embedding_data, batch_lengths, sorted_indices, unsorted_indices)\n",
    "      '''\n",
    "#       pad_seq, pad_seq_len = pad_packed_sequence(input_seq, batch_first=True)\n",
    "#       embed = self.get_concat_embedding(pad_seq)\n",
    "      embed = self.get_concat_embedding(input_seq.data)\n",
    "      embed_pack_seq = PackedSequence(embed, batch_sizes=input_seq.batch_sizes,\n",
    "                                      sorted_indices=input_seq.sorted_indices, unsorted_indices=input_seq.unsorted_indices)\n",
    "    \n",
    "      # 2. Put concatenated embedding to self.rnn\n",
    "      out, _ = self.rnn(embed_pack_seq)\n",
    "  \n",
    "      # 3. Put rnn output to self.final_layer to get probability logit for pitch and duration\n",
    "      # Again, rnn's output is PackedSequence so you have to handle it\n",
    "      # 3. rnn 출력을 self.final_layer에 넣어 피치 및 지속 시간에 대한 확률 로짓(probability logit)을 가져옵니다.\n",
    "      # 다시 말하지만 rnn의 출력은 PackedSequence이므로 사용자가 처리해야 합니다.\n",
    "      '''\n",
    "      1) out(PackedSequence) -> unpack_out(pad)\n",
    "      2) logit = self.final_layer(unpack_out)\n",
    "      '''\n",
    "      unpacked_out, unpacked_out_len = pad_packed_sequence(out, batch_first=True)\n",
    "      logits = self.final_layer(unpacked_out)\n",
    "      \n",
    "      # 4. Convert logits to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      # 4. 로짓들을 피치 확률과 지속 확률로 변환합니다.\n",
    "      # 조심하세요! 소프트맥스 피치와 지속시간을 따로 구하셔야 합니다.\n",
    "      # 확률 분포에서 피치 1개와 지속 시간 1개를 선택해야 하기 때문입니다.\n",
    "      \n",
    "      pitch_out = logits[..., :self.num_pitch]\n",
    "      dur_out = logits[..., self.num_pitch:]\n",
    "      pitch_out = torch.softmax(pitch_out, dim=-1)\n",
    "      dur_out = torch.softmax(dur_out, dim=-1)\n",
    "\n",
    "      # Return output as PackedSequence \n",
    "      packed_pitch_out = pack_padded_sequence(pitch_out, unpacked_out_len, batch_first=True)\n",
    "      packed_dur_out = pack_padded_sequence(dur_out, unpacked_out_len, batch_first=True)\n",
    "      \n",
    "      return packed_pitch_out, packed_dur_out\n",
    "\n",
    "    else:\n",
    "      print(f\"Unrecognized input type: {type(input_seq)}\")\n",
    "  \n",
    "\n",
    "hidden_size = 64\n",
    "embed_size = 40\n",
    "    \n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "108a8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your concart_embedding: \n",
      "tensor([[[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [ 1.7682,  0.7709,  0.8021,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         [ 0.3453, -0.3663,  0.6949,  ..., -0.3951, -0.7608, -0.4998],\n",
      "         ...,\n",
      "         [ 1.7682,  0.7709,  0.8021,  ...,  0.4306, -1.2245, -0.2752],\n",
      "         [ 0.4231,  0.2378, -0.4044,  ...,  0.4306, -1.2245, -0.2752],\n",
      "         [ 1.7682,  0.7709,  0.8021,  ..., -0.9295,  0.4493,  0.0180]],\n",
      "\n",
      "        [[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [ 0.4231,  0.2378, -0.4044,  ...,  0.4306, -1.2245, -0.2752],\n",
      "         [ 0.4231,  0.2378, -0.4044,  ..., -1.0564, -0.7562,  2.1363],\n",
      "         ...,\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119]],\n",
      "\n",
      "        [[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [-0.8960, -0.6369,  0.1142,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         [-0.5915,  1.0529, -1.6517,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         ...,\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [-0.6527, -0.5069,  0.0146,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         [-0.8578,  0.2134,  1.6378,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         ...,\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119]],\n",
      "\n",
      "        [[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [ 0.4231,  0.2378, -0.4044,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         [ 1.1260,  0.5749,  0.9157,  ...,  0.4306, -1.2245, -0.2752],\n",
      "         ...,\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119]],\n",
      "\n",
      "        [[-1.3129, -0.4756,  1.2102,  ..., -1.5082,  1.6598,  0.1772],\n",
      "         [-0.5915,  1.0529, -1.6517,  ..., -0.9295,  0.4493,  0.0180],\n",
      "         [ 0.4231,  0.2378, -0.4044,  ...,  0.4306, -1.2245, -0.2752],\n",
      "         ...,\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119],\n",
      "         [ 1.1975, -1.8345,  0.4201,  ...,  0.6955,  2.2746, -0.9119]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test model.get_concat_embedding\n",
    "'''\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "padded_melody, _ = pad_packed_sequence(melody, batch_first=True)\n",
    "\n",
    "concat_embedding = model.get_concat_embedding(padded_melody)\n",
    "print(f'Your concart_embedding: \\n{concat_embedding}')\n",
    "\n",
    "assert concat_embedding.shape[:-1] == padded_melody.shape[:-1], \"Num_batch and num_timestep of concat_embedding has to be the same with input melody\"\n",
    "assert concat_embedding.shape[2] == embed_size * 2, \"Error in size of embedding dimension\"\n",
    "assert (concat_embedding[0,0,:] == concat_embedding[1,0,:]).all(), \"Error: your embedding vectors for the same input notes are different\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd0050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with ordinary tensor (using batch_size=1)\n",
    "'''\n",
    "single_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "single_batch = next(iter(single_loader))\n",
    "single_melody, single_shifted_melody = single_batch\n",
    "# print(f\"[!]single_melody: {single_melody.shape}\")\n",
    "pitch_out, dur_out = model(single_melody)\n",
    "\n",
    "assert pitch_out.shape == (1,single_melody.shape[1], model.num_pitch),  \\\n",
    "          f\"Error in pitch_out.shape. Expected {1,single_melody.shape[1], model.num_pitch}, but got {pitch_out.shape}\"\n",
    "assert dur_out.shape == (1,single_melody.shape[1], model.num_dur), \\\n",
    "          f\"Error in dur_out.shape. Expected {1,single_melody.shape[1], model.num_dur}, but got {dur_out.shape}\"\n",
    "\n",
    "assert (0<pitch_out).all() and (pitch_out<1).all() and (0<dur_out).all() and (dur_out<1).all(), \\\n",
    "          \"Every output must have a value between 0 and 1 \"\n",
    "assert (torch.abs(torch.sum(pitch_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91f6c773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pitch_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a022349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with PackedSequence\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "pitch_out, dur_out = model(melody)\n",
    "\n",
    "assert isinstance(pitch_out, type(melody)) and isinstance(dur_out, type(melody)), f\"Input of model was {type(melody)} but output is {type(pitch_out)}\"\n",
    "\n",
    "assert (pitch_out.batch_sizes == melody.batch_sizes).all(), \\\n",
    "          \"batch_sizes of input and output has to be the same\"\n",
    "assert len(pitch_out.data) == len(batch[0].data), \"Number of notes in input and output has to be the same\"\n",
    "assert (torch.abs(torch.sum(pitch_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ec23",
   "metadata": {},
   "source": [
    "## Problem 6. Implement training loop (25 pts)\n",
    "- If you have succeeded in implementing model for PackedSequence, you can implement the training loop assuming that input batch is a PackedSequence\n",
    "- If not, you can implement the training loop using batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1d8d6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_distribution: \n",
      "tensor([[0.2287, 0.2227, 0.5487],\n",
      "        [0.1301, 0.4690, 0.4010],\n",
      "        [0.3269, 0.0541, 0.6190],\n",
      "        [0.0923, 0.4633, 0.4444],\n",
      "        [0.2072, 0.6336, 0.1592],\n",
      "        [0.3624, 0.2194, 0.4182],\n",
      "        [0.1773, 0.3721, 0.4505],\n",
      "        [0.6786, 0.2615, 0.0598],\n",
      "        [0.2339, 0.5722, 0.1939],\n",
      "        [0.0827, 0.1324, 0.7848]]), \n",
      " correct_class for each datasample: \n",
      " tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]])\n",
      "Loss:  tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403,\n",
      "        0.2423])\n"
     ]
    }
   ],
   "source": [
    "def get_cross_entropy_loss(prob_distribution, correct_class):\n",
    "  '''\n",
    "  This function takes predicted probability distrubtion and the corresponding correct_class.\n",
    "  \n",
    "  For example,  prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]] means that\n",
    "  for 0th data sample, the predicted probability for 0th category is 0.2287, for 1st category is 0.2227, and for 2nd category is 0.5487,\n",
    "  and for 1st data sample, the predicted probability for 0th category is 0.1301, for 1st category is 0.4690, and for 2nd category is 0.4010,\n",
    "  \n",
    "  Cross entropy, which is -y*log(y_hat), can be regarded as negative log value of predicted probability for correct class (y==1).\n",
    "  If the given correct_class is [1, 2], the loss for 0th data sample becomes negative log of [0.2287, 0.2227, 0.5487][1], which is -torch.log(0.2227), \n",
    "  because the correct category for this sample was 1st category, and the predicted probability was 0.2227\n",
    "  And the loss for 1st data sample becomes negative log of [0.1301, 0.4690, 0.4010][2], which is -torch.log(0.4010),\n",
    "  because the correct category for this sample was 2nd category, and the predicted probability was 0.4010\n",
    "  \n",
    "  To make implementation easy, let's assume we have 2D tensor for prob_distribution and  1D tensor for correct_class\n",
    "   \n",
    "  Arguments:\n",
    "    prob_distribution (2D Tensor)\n",
    "    correct_class (1D Tensor)\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Cross entropy loss for every data sample in prob_distrubition. Has a same shape with correct_class\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  Caution: When use torch.log(), don't forget to add small epsilon value (like 1e-6) to avoid infinity\n",
    "  Do not return the mean loss. Return loss that has same shape with correct_class\n",
    "  Try not to use for loop, or torch.nn.CrossEntropyLoss, or torch.nn.NLLLoss\n",
    "  \n",
    "  이 함수는 예측 확률 분포와 해당 correct_class를 사용합니다.\n",
    "  예를 들어 prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]]은 다음을 의미합니다.\n",
    "  0번째 데이터 표본의 경우 0번째 범주의 예측 확률은 0.2287, 1번째 범주의 예측 확률은 0.227, 2번째 범주의 예측 확률은 0.5487입니다.\n",
    "  첫 번째 데이터 표본의 경우 0번째 범주의 예측 확률은 0.1301, 첫 번째 범주의 경우 0.4690, 두 번째 범주의 경우 0.4010입니다.\n",
    "  \n",
    "  Cross entropy, 즉 -y*log(y_hat)는 올바른 클래스에 대한 예측 확률의 음수 로그 값(y==1)으로 간주할 수 있습니다.\n",
    "  주어진 correct_class가 [1, 2]이면 0번째 데이터 샘플에 대한 손실은 -torch.log(0.2227)인 [0.2287, 0.2227, 0.5487][1]의 음의 로그가 됩니다. \n",
    "  이 표본에 대한 올바른 범주는 첫 번째 범주이고 예측 확률은 0.227이기 때문입니다.\n",
    "  그리고 1번째 데이터 표본의 손실은 [0.1301, 0.4690, 0.4010][2]의 음의 로그가 됩니다. 이는 -torch.log(0.4010)입니다.\n",
    "  이 표본에 대한 올바른 범주는 두 번째 범주이고 예측 확률은 0.4010이기 때문입니다.\n",
    "  \n",
    "  구현을 쉽게 하기 위해 prob_distribution에 대한 2D 텐서와 correct_class에 대한 1D 텐서가 있다고 가정합니다.\n",
    "  '''\n",
    "  assert prob_distribution.dim() == 2 and correct_class.dim() == 1, \"Let's assume we only take 2D tensor for prob_distribution and 1D tensor for correct_class\"\n",
    "  # Write your code from here\n",
    "  cross_entropy_loss = []\n",
    "  \n",
    "  for idx in range(len(prob_distribution)):\n",
    "    loss = -torch.log(prob_distribution[idx, correct_class[idx]])\n",
    "    cross_entropy_loss.append(loss)\n",
    "  \n",
    "  return torch.stack(cross_entropy_loss)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "prob_distribution = torch.softmax(torch.randn([10, 3]), dim=-1)\n",
    "correct_class = torch.randint(0,3, [10])\n",
    "\n",
    "print(f\"prob_distribution: \\n{prob_distribution}, \\n correct_class for each datasample: \\n {correct_class.unsqueeze(1)}\")\n",
    "\n",
    "loss = get_cross_entropy_loss(prob_distribution, correct_class)\n",
    "print('Loss: ', loss)\n",
    "assert (torch.abs(loss-torch.Tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403, 0.2423]))<1e-4).all(), \"Error in loss value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "963172fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7381, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_for_single_batch(model, batch, device):\n",
    "  '''\n",
    "  This function takes model and batch and calculate Cross Entropy Loss for given batch.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    batch (batch collated by pack_collate): Tuple of (melody_batch, shifted_melody_batch)\n",
    "    device (str): cuda or cpu. In which device to calculate the batch\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Calculated mean loss for given model and batch\n",
    "    \n",
    "  TODO: Complete this function using get_cross_entropy_loss().\n",
    "  Now you have to return the mean loss of every data sample in the batch \n",
    "  \n",
    "  Caution: You have to calculate loss for pitch, and loss for duration separately.\n",
    "  Then you can take average of pitch_loss and duration_loss\n",
    "  \n",
    "  Important Tip: If you are using PackedSequence, you can feed PackedSequence.data directly to get_cross_entropy_loss.\n",
    "  It makes the implementation much easier, because it doesn't need to reshape probabilty distribution to 2D and correct class to 1D.\n",
    "  \n",
    "  TODO: get_cross_entropy_loss()를 사용하여 이 함수를 완료합니다.\n",
    "  이제 배치에 있는 모든 데이터 샘플의 평균 손실을 반환해야 합니다. \n",
    "  \n",
    "  주의: 당신은 pitch_loss와 duration_loss을 따로 계산해야 합니다.\n",
    "  그러면 pitch_loss와 duration_loss의 평균을 구할 수 있습니다.\n",
    "  \n",
    "  참고: PackedSequence를 사용하는 경우 PackedSequence.data를 get_cross_entropy_loss에 직접 공급할 수 있습니다.\n",
    "  확률 분포를 2D로 재구성하고 클래스를 1D로 수정할 필요가 없기 때문에 구현이 훨씬 쉬워집니다.\n",
    "  '''\n",
    "  melody_batch, shifted_melody_batch = batch\n",
    "  melody_batch = melody_batch.to(device)\n",
    "  shifted_melody_batch = shifted_melody_batch.to(device)\n",
    "\n",
    "  pred_pitch, pred_dur = model(melody_batch)\n",
    "\n",
    "  target_pitch, target_dur = shifted_melody_batch.data[..., 0], shifted_melody_batch.data[..., 1]\n",
    "  pred_pitch = pred_pitch.data.to(device)\n",
    "  pred_loss = pred_pitch.data.to(device)\n",
    "\n",
    "  pitch_loss = get_cross_entropy_loss(pred_pitch.data, target_pitch)\n",
    "  dur_loss = get_cross_entropy_loss(pred_dur.data, target_dur)\n",
    "#   return torch.stack([pitch_loss.mean(), dur_loss.mean()]).mean()\n",
    "#   pitch_loss = pitch_loss.to(device)\n",
    "#   print(pitch_loss)\n",
    "  return torch.mean((pitch_loss.float()+dur_loss.float())/2)\n",
    "\n",
    "######################################### TODO 제출 전 고치기 #########################################\n",
    "# DEV = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# DEV = torch.device('mps:0')\n",
    "# DEV = 'mps'\n",
    "DEV = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(DEV)\n",
    "batch = next(iter(train_loader))\n",
    "get_loss_for_single_batch(model, batch, device=DEV)\n",
    "\n",
    "# model.to('cuda')\n",
    "# batch = next(iter(train_loader))\n",
    "# get_loss_for_single_batch(model, batch, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9aeb02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052791a8348e4e439a1ff9615c12c2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68d3b658c2e408288696e95b4b2df7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d04143951d4e6396d07d17a8075735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7d66db77264ef983d0b8ad445623ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204103ee2fb849ccbb5e4788ef9d9ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd4fff7a0604e7da8cd75aa4a56753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d80696eb1f462eb83d3456d7402665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c49338b0fa3418b984c76881049c6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7fc4b2f64d41f2ad271b5628bb264a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0299656acdc848059ed60bf4ffddfd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77256be565044a6d919b7f7d621b38be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7254980cadab4a99833700cf635a1f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb7a795e41143adacca99a2d8c2aa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a68871c3f6a426e8512ecf677c8a061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e56bc062f4840825c213886f4cdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2657098a3aa6449ca1136f84ae68dcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ea0ec2bbd46459fd67daebbae7ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84f9ae4f10d4555b456ec29169a7688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c184312f1b6e41bd9c3f4923aec2a2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551225f597f40d3bdc9e366ed1725b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb88a4241884d3992403df91f19a699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24aa0a8fae24cb5982baa88f2db0d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bb39b254fb4501900d2b59a5281f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c68857958754dcbbc21f2acf20600d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a79995cf30248a196829fc1e5e5efe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb69a513c6e4f7ca79f78408cc56370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66621fb75e343869c3b095a27649f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b8b392413f4dfb897f7e22d2bb54ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0356b965fdd46dfa7ec5cd2de24f12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf51250e3974e7fb3f4972b23e97324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c567d86c78439798b44e6199c7b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "If you have implemented the previous function correctly, this code will train the model\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "######################################### TODO: 고치기 #########################################\n",
    "# DEV = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# DEV = 'mps'\n",
    "DEV = 'cuda' if torch.cuda.is_available() else 'cpu'# or cpu, but using cpu will be too slow\n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 30\n",
    "\n",
    "model.to(DEV)\n",
    "loss_record = []\n",
    "valid_loss_record = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader,leave=False):\n",
    "    loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_record.append(loss.item())\n",
    "    \n",
    "  # Validation\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss_for_entire_valid = 0\n",
    "    num_notes = 0\n",
    "    for batch in valid_loader:\n",
    "      loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "      if isinstance(batch[0], PackedSequence):\n",
    "        n_note = len(batch[0].data)        \n",
    "      else:\n",
    "        n_note = batch[0].shape[1]\n",
    "        \n",
    "      loss_for_entire_valid += loss.item() * n_note\n",
    "      num_notes += n_note\n",
    "    valid_loss_record.append(loss_for_entire_valid/num_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4be25b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ab85a970>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 589,
       "width": 605
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(loss_record)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Validation Loss')\n",
    "plt.plot(valid_loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa5d108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/MIR_HW3-1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3c6f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./models/MIR_HW3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ed2",
   "metadata": {},
   "source": [
    "## Problem 7: Implement Generation (25 pts)\n",
    "- In this problem, you have to generate a new melody using the trained model\n",
    "- Melody language model can generate a new sequence by sampling a new note for each timestep, and feed the generated new note again to the model to predict the next note\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd915d",
   "metadata": {},
   "source": [
    "### Problem 7-1: Implement model inference (15 pts)\n",
    "- Inference in Language model is little bit different from an ordniary forward loop during the training.\n",
    "    - While training, you have entire sequence, from beginning to end.\n",
    "    - During the inference, you have to generate one note, and then feed it as an input for the next step\n",
    "- You have to implement given functions one by one to complete `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2afa1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vec: \n",
      "tensor([[[38., 44.]],\n",
      "\n",
      "        [[38., 44.]]]) \n",
      " initial_hidden: \n",
      " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def get_initial_input_and_hidden_state(model, batch_size=1):\n",
    "  '''\n",
    "  This function generates initial input vector and hidden state for model's GRU\n",
    "  \n",
    "  To generate a new sequence, you have to provide initial seed token, which is ['start', 'start'].\n",
    "  You have to make a initial vector that has [pitch_category_index_of_'start', duration_category_index_of_'start']\n",
    "  \n",
    "  You also have to initial hidden state for the model's RNN.\n",
    "  In uni-directional RNN(or GRU), hidden state of RNN has to be a zero tensor with shape of (num_layers, batch_size, hidden_size)\n",
    "\n",
    "  \n",
    "  Argument:\n",
    "    model (MelodyLanguageModel)\n",
    "    \n",
    "  Returns:\n",
    "    initial_input_vec (torch.Tensor): Has a shape of [batch_size, 1 (timestep), 2]\n",
    "    initial_hidden (torch.Tensor): Has a shape of [num_layers, bach_size, hidden_size]\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  이 함수는 모델의 GRU에 대한 초기 입력 벡터와 숨겨진 상태를 생성합니다.\n",
    "  \n",
    "  새 시퀀스를 생성하려면 초기 시드 토큰(['start', 'start'])을 제공해야 합니다.\n",
    "  초기 벡터는 [pitch_category_index_of_'start', duration_category_index_of_'start']로 만들어야 합니다.\n",
    "  \n",
    "  모델의 RNN에 대한 초기 숨김 상태도 지정해야 합니다.\n",
    "  단방향 RNN(또는 GRU)에서 RNN의 숨겨진 상태는 (num_layers, batch_size, hidden_size)의 모양을 가진 0 텐서여야 합니다.\n",
    "\n",
    "  '''\n",
    "  initial_token = [[model.pitch2idx['start'], model.dur2idx['start']]]\n",
    "  initial_input_vec = torch.Tensor([initial_token]*batch_size)\n",
    "\n",
    "  initial_hidden = torch.zeros((model.num_layers, batch_size, model.hidden_size))\n",
    "  return initial_input_vec, initial_hidden\n",
    "\n",
    "batch_size = 2\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=batch_size)\n",
    "print(f'input_vec: \\n{input_vec} \\n initial_hidden: \\n {initial_hidden}')\n",
    "\n",
    "assert input_vec.ndim == 3\n",
    "assert initial_hidden.ndim == 3\n",
    "assert input_vec.shape == (batch_size, 1, 2)\n",
    "assert initial_hidden.shape == (model.num_layers, batch_size, model.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c5e91",
   "metadata": {},
   "source": [
    "### Hint: Sampling from distribution\n",
    "- The language model predict probability distribution of pitch and duration for  upcoming note\n",
    "- To do that, you have to know how to sample a result from a given probability distribution\n",
    "- In PyTorch, you can use `atensor.multinomial(num_samples)`\n",
    "    - In this assignment you don't have to sample more than 1, but \n",
    "    - multinomial(num_samples=100, replacement=False) means that you want to sample 100 samples without overlapping category\n",
    "        - Thus, the total class has to be larger than 100, because you cannot sample a single category multiple time\n",
    "    - multinomial(num_samples=100, replacement=True) means that you will sample 100 from the distrubtion independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23845568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 1, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({4: 1576, 2: 2035, 1: 4989, 3: 438, 0: 962})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "'''\n",
    "Example of sampling a result from a given probability distribution\n",
    "'''\n",
    "\n",
    "dummy_prob_distribution = torch.Tensor([0.1, 0.5, 0.2, 0.05, 0.15])\n",
    "sampled_out = dummy_prob_distribution.multinomial(num_samples=10000, replacement=True)\n",
    "print(sampled_out[:20])\n",
    "Counter(sampled_out.tolist()) # Number of each category sampled is almost same as num_samples * probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15fef28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_note: \n",
      "tensor([[[ 2., 10.]]]) \n",
      " last_hidden: \n",
      " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def predict_single_step(model, cur_input, prev_hidden):\n",
    "  '''\n",
    "  This function runs MelodyLangaugeModel just for one step, for the given current input and previous hidden state.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_input (torch.LongTensor): Input for the current time step. Has a shape of (batch_size=1, 1 (timestep), 2)\n",
    "    prev_hidden (torch.Tensor): Hidden state of RNN after previous timestep\n",
    "\n",
    "  Returns:\n",
    "    cur_output (torch.LongTensor): Sampled note [pitch_category_idx, duration_category_idx] from the predicted probability distribution, with shape of [1,1,2]\n",
    "    last_hidden (torch.Tensor): Hidden state of RNN\n",
    "  Think about running the model.forward() step-by-step.\n",
    "  \n",
    "  input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration] → sampled [pitch, duration]\n",
    "  \n",
    "  ######\n",
    "  이 함수는 주어진 current input과 previous hidden state에 대해 1 step동안 MelodyLanguageModel을 실행합니다.\n",
    "  Returns:\n",
    "    cur_output (torch.LongTensor): 예측 확률 분포에서 [1,1,2] 모양의 노트 [pitch_category_idx, duration_category_idx]를 샘플링\n",
    "    last_hidden (torch.Tensor): Hidden state of RNN\n",
    "\n",
    "  '''\n",
    "  out_pitch, out_dur = model.forward(cur_input.long())\n",
    "\n",
    "  prob_pitch, idx_pitch = out_pitch.max(2)\n",
    "  prob_dur, idx_dur = out_dur.max(2)\n",
    "  \n",
    "  cur_output = torch.Tensor([[[idx_pitch, idx_dur]]])\n",
    "  \n",
    "#   pitch = model.idx2pitch[idx_pitch]\n",
    "#   dur = model.idx2dur[idx_dur]\n",
    "\n",
    "#   cur_output = torch.Tensor([[[pitch, dur]]])\n",
    "  last_hidden = prev_hidden\n",
    "  return cur_output, last_hidden\n",
    "\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=1)\n",
    "out_note, last_hidden = predict_single_step(model, input_vec, initial_hidden)\n",
    "print(f'out_note: \\n{out_note} \\n last_hidden: \\n {last_hidden}')\n",
    "\n",
    "assert out_note.ndim == 3\n",
    "assert last_hidden.ndim == 3\n",
    "assert out_note.shape == (1,1,2)\n",
    "\n",
    "assert len(set([predict_single_step(model, input_vec, initial_hidden)[0] for i in range(5)]))==5, 'Generated output has to be different based on random sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d87107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_end_token(model, cur_output):\n",
    "  '''\n",
    "  During the generation, there is a possibility that the generated note predicted 'end' token for either pitch or duration.\n",
    "  (In fact, model can even estimate 'start' token during the generation even though it has very low probability)\n",
    "  \n",
    "  Using information among (model.pitch2idx, model.dur2idx, model.idx2pitch, model.idx2dur, model.num_pitch, model.num_dur), check whether \n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_output (torch.LongTensor): Assume it has shape of [1,1,2 (pitch_idx, duration_idx)]\n",
    "  \n",
    "  Return:\n",
    "    is_end_token (bool): True if cur_output include category index such as 'start' or 'end',\n",
    "                          else False.\n",
    "                          \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  idx_pitch = cur_output[..., 0].long()\n",
    "  idx_dur = cur_output[..., 1].long()\n",
    "    \n",
    "  pitch = model.idx2pitch[idx_pitch]\n",
    "  dur = model.idx2dur[idx_dur]\n",
    "  \n",
    "  is_end_token = False\n",
    "  if pitch == 'start' or pitch == 'end' or dur == 'start' or dur == 'end':\n",
    "    is_end_token = True\n",
    "    \n",
    "  return is_end_token\n",
    "\n",
    "print(is_end_token(model, out_note))\n",
    "\n",
    "assert not is_end_token(model, torch.LongTensor([[[10, 7]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[38, 40]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[25, 44]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[39, 41]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c079ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2., 10.]]])\n",
      "tensor([[[35., 10.]]])\n",
      "tensor([[[39., 10.]]])\n",
      "gen_out: \n",
      " tensor([[ 2, 10],\n",
      "        [35, 10]])\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "def generate(model, random_seed=0):\n",
    "  '''\n",
    "  This function generates a new melody sequence with a given model and random_seed.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    random_seed (int): Language model's inference will always generate different result, because it uses random sampling for the prediction.\n",
    "                       Therefore, if you want to reproduce the same generation result, you have to fix random_seed.\n",
    "  \n",
    "  Returns:\n",
    "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
    "  \n",
    "  TODO: Complete this function using get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()\n",
    "  \n",
    "  Hint: You can use while loop\n",
    "        You have to track the generated single note in a list or somewhere. \n",
    "        \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    random_seed(int): 언어 모델의 추론은 예측에 랜덤 표본을 사용하기 때문에 항상 다른 결과를 생성합니다.\n",
    "                       따라서 동일한 생성 결과를 재현하려면 random_seed를 고정해야 합니다.\n",
    "  \n",
    "  Returns:\n",
    "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
    "  \n",
    "  TODO: get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()을 사용하여 이 함수를 완료합니다.\n",
    "  \n",
    "  [!] 힌트: while loop을 사용할 수 있습니다.\n",
    "        생성된 단일 노트를 목록이나 다른 곳에서 추적해야 합니다.\n",
    "  '''\n",
    "  \n",
    "  torch.manual_seed(random_seed) # To reproduce the result, we have to control random sequence\n",
    "  \n",
    "  '''\n",
    "  Write your code from here\n",
    "  '''\n",
    "  input_vec, initial_hidden = get_initial_input_and_hidden_state(model)\n",
    "  out_note, last_hidden = input_vec, initial_hidden\n",
    "  generated_note_sequence = []\n",
    "  while True:\n",
    "    out_note, last_hidden = predict_single_step(model, out_note, last_hidden)\n",
    "    print(out_note)\n",
    "    if is_end_token(model, out_note):\n",
    "      break\n",
    "    generated_note_sequence.append(out_note.flatten().long().tolist())\n",
    "\n",
    "      \n",
    "  return torch.LongTensor(generated_note_sequence)\n",
    "\n",
    "gen_out = generate(model)\n",
    "print(f\"gen_out: \\n {gen_out}\")\n",
    "\n",
    "assert isinstance(gen_out, torch.LongTensor), f\"output of generate() has to be torch.LongTensor, not {type(gen_out)}\"\n",
    "assert gen_out.ndim == 2, f\"output of generate() has to be 2D tensor, not {gen_out.ndim}D tensor\"\n",
    "assert gen_out.shape[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681a9e5",
   "metadata": {},
   "source": [
    "### Problem 7-2. Convert neural network's prediction to music score (10 pts)\n",
    "- Even though neural network has succeeded in generating a new sequence, it is just a sequence of index that neural network uses\n",
    "    - For example, generated note event [17, 10] means that this note has pitch value of 17th pitch category and duration value of 10th duration category\n",
    "- We have to convert categorical index to original value\n",
    "    - We saved this information as `idx2pitch`, `idx2dur` while we declared the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f6f18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_pred_to_origin(pred:torch.Tensor, idx2pitch:list, idx2dur:list):\n",
    "  '''\n",
    "  This function convert neural net's output index to original pitch value (MIDI Pitch) and duration value \n",
    "  \n",
    "  Argument:\n",
    "    pred: generated output of the model. Has a shape of [num_notes, 2]. \n",
    "          0th dimension of each note represents pitch category index \n",
    "          and 1st dimension of each note represents duration category index\n",
    "  \n",
    "  Return:\n",
    "    converted_out (torch.Tensor): Has a same shape with 'pred'.\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  converted_out = []\n",
    "  for x in pred:\n",
    "    pitch, dur = idx2pitch[x[0]], idx2dur[x[1]]\n",
    "    converted_out.append([pitch, dur])\n",
    "  return torch.Tensor(converted_out)\n",
    "\n",
    "converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "assert converted_out.shape == gen_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2be29831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   72,   24,   64],\n",
       "       [  24,   72,   24,   64],\n",
       "       [  48,   77,   12,   64],\n",
       "       [  60,   72,   24,   64],\n",
       "       [  84,   70,   12,   64],\n",
       "       [  96,   67,   12,   64],\n",
       "       [ 108,   72,   12,   64],\n",
       "       [ 120,   63,   12,   64],\n",
       "       [ 132,   65,   12,   64],\n",
       "       [ 144,   67,   48,   64],\n",
       "       [ 192,   72,   24,   64],\n",
       "       [ 216,   72,   24,   64],\n",
       "       [ 240,   77,   12,   64],\n",
       "       [ 252,   72,   24,   64],\n",
       "       [ 276,   70,   12,   64],\n",
       "       [ 288,   67,   12,   64],\n",
       "       [ 300,   72,   12,   64],\n",
       "       [ 312,   63,   12,   64],\n",
       "       [ 324,   65,   12,   64],\n",
       "       [ 336,   67,   48,   64],\n",
       "       [ 384,   77,   12,   64],\n",
       "       [ 396,   74,   12,   64],\n",
       "       [ 408,   77,   18,   64],\n",
       "       [ 426,   74,    6,   64],\n",
       "       [ 432,   72,   12,   64],\n",
       "       [ 444,   77,   12,   64],\n",
       "       [ 456,   74,   12,   64],\n",
       "       [ 468,   77,   12,   64],\n",
       "       [ 480,   72,   24,   64],\n",
       "       [ 504,   70,   12,   64],\n",
       "       [ 516,   67,   12,   64],\n",
       "       [ 528,   72,   48,   64],\n",
       "       [ 576,   67,   12,   64],\n",
       "       [ 588,   67,    6,   64],\n",
       "       [ 594,   67,    6,   64],\n",
       "       [ 600,   67,   12,   64],\n",
       "       [ 612,   72,   12,   64],\n",
       "       [ 624,   67,   12,   64],\n",
       "       [ 636,   65,   12,   64],\n",
       "       [ 648,   63,   12,   64],\n",
       "       [ 660,   62,   12,   64],\n",
       "       [ 672,   60,   48,   64],\n",
       "       [ 720,   65,   12,   64],\n",
       "       [ 732,   63,   24,   64],\n",
       "       [ 756,   65,   12,   64],\n",
       "       [ 768,   67,   48,   64],\n",
       "       [ 816,   72,   48,   64],\n",
       "       [ 864,   67,   24,   64],\n",
       "       [ 888,   69,   12,   64],\n",
       "       [ 900,   67,   12,   64],\n",
       "       [ 912,   65,   36,   64],\n",
       "       [ 948,   69,   12,   64],\n",
       "       [ 960,   67,   12,   64],\n",
       "       [ 972,   65,   12,   64],\n",
       "       [ 984,   63,   12,   64],\n",
       "       [ 996,   62,   12,   64],\n",
       "       [1008,   60,   48,   64]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To solve the next problem, you have to know how note_representation looks like in muspy.\n",
    "\n",
    "In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "'''\n",
    "note_repr_example = train_set.dataset[0]\n",
    "note_repr_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e067efdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64,  50,  12,  64],\n",
       "       [128,  83,  12,  64]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_pitch_dur_to_note_representation(pitch_dur:torch.LongTensor):\n",
    "  '''\n",
    "  This function takes pitch_dur (shape of [num_notes, 2]) and returns the corresponding note representation (shape of [num_notes, 4])\n",
    "  In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "  \n",
    "  Since our generation is monophonic, you can regard start_timestep starts from 0 and accumulate the duration of note.\n",
    "  You can fix velocity to 64.\n",
    "  \n",
    "  \n",
    "  Arguments:\n",
    "    pitch_dur: LongTensor of note where each note represented as pitch and duration value\n",
    "    \n",
    "  return:\n",
    "    note_repr: numpy.Array with shape of [num_notes, 4]\n",
    "               each note has value of [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "  TODO: Complete this function\n",
    "  Hint: You can use torch.cumsum() to accumulate the duration.\n",
    "  To convert torch tensor to numpy, you can use atensor.numpy()\n",
    "  \n",
    "  이 함수는 pitch_dur([num_notes, 2]의 모양)를 사용하고 해당 음표 표현([num_notes, 4]의 모양)을 반환합니다.\n",
    "  노트 표현에서 각 노트는 [start_timestep, 피치, 지속 시간, 속도]로 표현됩니다.\n",
    "  \n",
    "  우리 세대는 단음파이기 때문에 start_timestep starts를 0에서 시작하여 음의 지속시간을 누적시킬 수 있습니다.\n",
    "  속도를 64로 고정할 수 있습니다.\n",
    "  \n",
    "  \n",
    "  인수:\n",
    "    pitch_dur: 각 음이 피치 및 지속 시간 값으로 표시되는 노트의 LongTensor입니다.\n",
    "    \n",
    "  반환하십시오.\n",
    "    note_repr: numpy입니다.모양이 [num_notes, 4]인 배열입니다.\n",
    "               각 노트의 값은 [start_contip, 피치, 지속 시간, 속도]입니다.\n",
    "\n",
    "  TODO: 이 기능을 완료합니다.\n",
    "  힌트: torch.cumsum()을 사용하여 기간을 누적할 수 있습니다.\n",
    "  토치 텐서를 numpy로 변환하려면 텐서를 사용할 수 있습니\n",
    "  \n",
    "  '''\n",
    "#   print(pitch_dur)\n",
    "  timestep = torch.cumsum(torch.Tensor([64]*len(pitch_dur)), dim=0)\n",
    "  velocity = torch.Tensor([64]*len(pitch_dur))\n",
    "  note_repr = torch.stack([timestep, pitch_dur[...,0], pitch_dur[...,1], velocity], dim=-1).long()\n",
    "#   print(f'note_repr: {note_repr}')\n",
    "  return note_repr.numpy()\n",
    "\n",
    "note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "note_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d9303",
   "metadata": {},
   "source": [
    "## Generation: Visualize and synthesize the generated result (10 pts)\n",
    "- Try to generate different melody using different `random_seed`\n",
    "- In your submission, include **Three** examples of your favorite among the generated results in wav\n",
    "    - You have to install soundfont and music font using \n",
    "        - `muspy.download_bravura_font()`\n",
    "        - `muspy.download_musescore_soundfont()`\n",
    "    - You may need fluidsynth to synthesize the sound.\n",
    "        - In colab, `!sudo apt-get install fluidsynth` will work\n",
    "        - In other Ubuntu os, `sudo apt-get update` and then `sudo apt-get install fluidsynth` will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71a3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "gen_music = muspy.from_note_representation(note_repr)\n",
    "gen_music.show_score()\n",
    "\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9a3cb",
   "metadata": {},
   "source": [
    "- Try with different random seed and generate interesting melodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f300bd4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fluidsynth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [254]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m gen_music \u001b[38;5;241m=\u001b[39m generate_muspy_music(model, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m gen_music\u001b[38;5;241m.\u001b[39mshow_score()\n\u001b[0;32m---> 13\u001b[0m gen_audio \u001b[38;5;241m=\u001b[39m \u001b[43mgen_music\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     14\u001b[0m ipd\u001b[38;5;241m.\u001b[39mAudio(gen_audio\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m15\u001b[39m, rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44100\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/muspy/music.py:652\u001b[0m, in \u001b[0;36mMusic.synthesize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynthesize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;124;03m\"\"\"Synthesize a Music object to raw audio.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    Refer to :func:`muspy.synthesize` for full documentation.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/muspy/outputs/audio.py:72\u001b[0m, in \u001b[0;36msynthesize\u001b[0;34m(music, soundfont_path, rate, gain)\u001b[0m\n\u001b[1;32m     69\u001b[0m     write_midi(midi_path, music)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Synthesize the MIDI file using fluidsynth\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfluidsynth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-T\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-F-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-r\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-i\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoundfont_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmidi_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Decode bytes to waveform\u001b[39;00m\n\u001b[1;32m     91\u001b[0m waveform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(result\u001b[38;5;241m.\u001b[39mstdout, np\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    503\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:1821\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1820\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1821\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fluidsynth'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/events.py:89: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  func(*args, **kwargs)\n",
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/events.py:89: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  func(*args, **kwargs)\n",
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 108 (l) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/homebrew/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 112 (p) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 277
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def generate_muspy_music(model, random_seed=0):\n",
    "  '''\n",
    "  This function combines 'generate', 'convert_idx_pred_to_origin', 'convert_pitch_dur_to_note_representation', muspy.from_note_representation\n",
    "  '''\n",
    "  gen_out = generate(model, random_seed)\n",
    "  converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "  note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "  gen_music = muspy.from_note_representation(note_repr)\n",
    "  return gen_music\n",
    "\n",
    "gen_music = generate_muspy_music(model, random_seed=2)\n",
    "gen_music.show_score()\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can save audio as wave file with muspy.write_audio\n",
    "'''\n",
    "gen_music.write_audio('result_0.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
