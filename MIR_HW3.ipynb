{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bbdb28",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "- In this assignment, you will implement a melody-language model\n",
    "- You have to submit your code in **TWO** formats:\n",
    "    - Completed Notebook with `.ipynb`\n",
    "    - A `{your_student_number}.py` file that includes **ALL functions and classes you have completed**\n",
    "        - Do not include any other code except function and class\n",
    "        - Your result will be scored by an evaluation code that import this `{your_student_number}.py` file\n",
    "        - So be careful not to use any global variable inside the function\n",
    "- You have to submit a report (optional) and **three** generation results of your favorite in wav files\n",
    "    - The report is optional. If you have tried other architecture for MelodyLanguage Model, you can describe the result.\n",
    "\n",
    "\n",
    "- Caution: The `assert` lines are designed to check whether basic requirements are satisfied. Even though you passed all the assert cases, it doesn't guarantee that your implementation is fully correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24cd5c",
   "metadata": {},
   "source": [
    "## 0. Prepare (Install and import library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e670ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: muspy in /opt/homebrew/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: mido>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.2.10)\n",
      "Requirement already satisfied: pretty-midi>=0.2 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.2.9)\n",
      "Requirement already satisfied: miditoolkit>=0.1 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.1.16)\n",
      "Requirement already satisfied: bidict>=0.21 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (0.22.0)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (2.27.1)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (6.0)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (3.5.2)\n",
      "Requirement already satisfied: joblib>=0.15 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.1.0)\n",
      "Requirement already satisfied: pypianoroll>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (1.0.4)\n",
      "Requirement already satisfied: tqdm>=4.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (4.64.0)\n",
      "Requirement already satisfied: music21>=6.0 in /opt/homebrew/lib/python3.9/site-packages (from muspy) (7.3.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (9.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=1.5->muspy) (3.0.9)\n",
      "Requirement already satisfied: chardet in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (4.0.0)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (8.13.0)\n",
      "Requirement already satisfied: jsonpickle in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (2.2.0)\n",
      "Requirement already satisfied: webcolors>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from music21>=6.0->muspy) (1.11.1)\n",
      "Requirement already satisfied: six in /opt/homebrew/lib/python3.9/site-packages (from pretty-midi>=0.2->muspy) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/homebrew/lib/python3.9/site-packages (from pypianoroll>=1.0->muspy) (1.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from requests>=2.0->muspy) (2.0.12)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSkip downloading as the Bravura font is found.\n",
      "Skip downloading as the MuseScore General soundfont is found.\n"
     ]
    }
   ],
   "source": [
    "!pip install muspy\n",
    "import muspy\n",
    "\n",
    "muspy.download_bravura_font()\n",
    "'''\n",
    "You may have to install fluidsynth.\n",
    "In Colab, you can install by followign code\n",
    "\n",
    "!sudo apt-get install fluidsynth\n",
    "'''\n",
    "muspy.download_musescore_soundfont()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9b3ba",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding and Implementing RNN (15 pts)\n",
    "- Recurrent neural network is a typical choice for handling sequential data with a neural network\n",
    "- In this problem, you have to implement a Vanilla RNN\n",
    "    - For each time step $t$, RNN takes two inputs\n",
    "        - $x_t$, which is an input vector of time step $t$\n",
    "        - $h_{t-1}$, which is an hidden state of previous time step, $t-1$\n",
    "            - $h_{t-1}$ is also the output of RNN for previous time step $t-1$\n",
    "    - For given $x_t$ and $h_{t-1}$, RNN returns $h_t$\n",
    "        - $h_t = \\tanh(Wx_t + Uh_{t-1} + b)$ \n",
    "            - $W$ and $U$ is a trainable weight matrix of RNN\n",
    "            - $W \\in \\mathbb{R}^{d \\times h}$, and $U \\in \\mathbb{R}^{h \\times h}$, where $d$ is number of input dimension and $h$ is number of hidden state dimension\n",
    "                - This means that $W$ is a matrix with real numbers and size of $\\text{num_input_dim}\\times \\text{num_hidden_dim}$  \n",
    "                - and $U$ is a matrix with real numbers and size of $\\text{num_hidden_dim}\\times \\text{num_hidden_dim}$  \n",
    "            \n",
    " - The output of fully connected layer (`nn.Linear`) for a given input vector $x$ is as below:\n",
    "     - $\\text{output} = Wx+b$\n",
    "     - Where $W$ is a weight matrix and $b$ is a bias vector\n",
    "     - Both $W$ and $b$ are trainable parameters\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050d35e",
   "metadata": {},
   "source": [
    "### Problem 1.1: Calculating Forward Propagation of RNN\n",
    "- Based on the example above, implement the forward propagation of uni-directional, single layer vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3564e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_weight_for_hidden_to_hidden: \n",
      " tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959],\n",
      "        [ 0.5667,  0.7935,  0.4397,  0.1124,  0.6408,  0.4412],\n",
      "        [-0.2159, -0.7425,  0.5627,  0.2596,  0.5229,  2.3022],\n",
      "        [-1.4689, -1.5867,  1.2032,  0.0845, -1.2001, -0.0048]])\n",
      "example_weight_for_input_to_hidden: \n",
      " tensor([[-0.2303, -0.3918, -0.4731],\n",
      "        [ 0.3356,  1.5091,  2.0820],\n",
      "        [ 1.7067,  2.3804, -1.1256],\n",
      "        [-0.3170, -0.1407,  0.8058],\n",
      "        [ 0.3276, -0.7607, -1.5991],\n",
      "        [ 0.0185, -0.7504,  0.1854]])\n",
      "example_bias: \n",
      " tensor([-0.6776,  1.0422, -1.9513,  0.4186,  3.3214,  0.8764])\n",
      "example_input_sequence: \n",
      " tensor([[ 0.3446,  0.5199, -2.6133],\n",
      "        [-1.6965, -0.2282,  0.2800],\n",
      "        [ 0.0732,  1.1133,  0.3380],\n",
      "        [ 0.4544,  0.4569, -0.8654],\n",
      "        [ 0.7813, -0.9268,  0.2064],\n",
      "        [-0.3334, -0.0729, -0.0340],\n",
      "        [ 0.9625,  0.3492, -0.9215],\n",
      "        [-0.0562, -0.7015,  1.0367],\n",
      "        [ 1.9218, -0.4025,  0.1239],\n",
      "        [ 1.1648,  0.9234,  1.3873],\n",
      "        [ 1.3750,  0.6596, -0.8048],\n",
      "        [ 0.5656,  0.6104,  0.4669],\n",
      "        [ 1.9507, -1.0631,  1.1404],\n",
      "        [-0.0899, -0.5940, -1.2439],\n",
      "        [-0.1021, -1.0335, -0.1434],\n",
      "        [-0.3173,  0.9671, -0.9911],\n",
      "        [ 0.3016, -0.1073,  0.9985],\n",
      "        [-0.4987,  0.9910, -0.7777],\n",
      "        [ 0.3140,  0.2133, -0.1201],\n",
      "        [ 0.3605, -0.3140, -1.0787]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Don't change this cell\n",
    "'''\n",
    "example_input_size = 3\n",
    "example_hidden_size = 6\n",
    "example_sequence_length = 20\n",
    "\n",
    "torch.manual_seed(0)\n",
    "example_weight_for_hidden_to_hidden = torch.randn([example_hidden_size, example_hidden_size])\n",
    "example_weight_for_input_to_hidden = torch.randn([example_hidden_size, example_input_size])\n",
    "example_bias = torch.randn([example_hidden_size])\n",
    "example_input_sequence = torch.randn([example_sequence_length, example_input_size])\n",
    "\n",
    "print('example_weight_for_hidden_to_hidden: \\n',example_weight_for_hidden_to_hidden)\n",
    "print('example_weight_for_input_to_hidden: \\n',example_weight_for_input_to_hidden)\n",
    "print('example_bias: \\n',example_bias)\n",
    "print('example_input_sequence: \\n',example_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_single_step(current_input:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> torch.Tensor:\n",
    "  '''\n",
    "  This function \n",
    "  \n",
    "  Arguments:\n",
    "    current_input: Input vector of the current time step. Has a shape of [input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  Outputs:\n",
    "    current hidden: Updated hidden state for the current time step. Has a shape of [hidden_dimension]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  â„Žð‘¡=tanh(ð‘Šð‘¥ð‘¡+ð‘ˆâ„Žð‘¡âˆ’1+ð‘)\n",
    "  '''\n",
    "  return torch.tanh(ih_weight@current_input + hh_weight@prev_hidden + bias)\n",
    "\n",
    "\n",
    "def initialize_hidden_state_for_single_batch(hidden_dim:int) -> torch.Tensor:\n",
    "  '''\n",
    "  This function returns zero Tensor for a given hidden dimension. This function assumes that the RNN uses single layer and single direction.\n",
    "  \n",
    "  Argument\n",
    "    hidden_dim\n",
    "    \n",
    "  Return\n",
    "    initial_hidden_state: Has a shape of [hidden_dim]\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  return torch.Tensor(hidden_dim)\n",
    "\n",
    "\n",
    "initial_hidden = initialize_hidden_state_for_single_batch(example_hidden_size)\n",
    "assert initial_hidden.shape == torch.Size([example_hidden_size])\n",
    "\n",
    "single_output = rnn_single_step(example_input_sequence[0], initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)\n",
    "assert (torch.abs(single_output - torch.Tensor([ 0.2690, -0.9982,  0.9929, -0.9535,  1.0000,  0.0081]))<1e-4).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_for_entire_timestep(input_seq:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> tuple:\n",
    "  '''\n",
    "  This function returns the output of RNN for the given 'input_seq', for the given RNN's parameters (hh_weight, ih_weight, and bias)\n",
    "  \n",
    "  Arguments:\n",
    "    input_seq: Sequence of input vector. Has a shape of [number_of_timestep, input_dimension]\n",
    "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
    "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
    "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
    "\n",
    "  \n",
    "  Return: tuple (output, final_hidden_state)\n",
    "    output: Sequence of output hidden state of RNN along input timesteps. Has a a shape of [number_of_timestep, hidden_dimension]\n",
    "    final_hidden_state: Hidden state of RNN of the last time step. Has a a shape of [hidden_dimension]\n",
    "    \n",
    "  TODO: Complete this function using your 'rnn_single_step()'\n",
    "  '''\n",
    "  output = []\n",
    "  for idx in range(input_seq.shape[0]):\n",
    "    curr_input = input_seq[idx] #\n",
    "    curr_hidden = rnn_single_step(curr_input, prev_hidden, hh_weight, ih_weight, bias)\n",
    "    prev_hidden = curr_hidden\n",
    "    output.append(curr_hidden)\n",
    "    \n",
    "  final_hidden_state = curr_hidden\n",
    "  output = torch.stack(output)\n",
    "  return (output, final_hidden_state)\n",
    "\n",
    "total_output = rnn_for_entire_timestep(example_input_sequence, initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6557792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test case\n",
    "'''\n",
    "\n",
    "assert isinstance(total_output, tuple) and len(total_output)==2, \"RNN's output has to be tuple of two tensors\"\n",
    "assert isinstance(total_output[0], torch.Tensor), 'Hidden states has to be a tensor'\n",
    "assert (total_output[0][6] - torch.Tensor([ 0.8273,  0.5121, -0.5701, -0.9566,  0.9984,  0.5125])).abs().min() < 1e-4, f\"Output value is different: {total_output[0][6]}\"\n",
    "assert (total_output[1]- torch.Tensor([-0.2121, -0.9892, -0.9953,  0.7993,  1.0000, -0.9995])).abs().min() < 1e-4, f\"Output value is different: {total_output[1]}\"\n",
    "\n",
    "print(\"Passed the test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3661e",
   "metadata": {},
   "source": [
    "## Problem 2: Understanding Embedding Layer (10 pts)\n",
    "- Embedding Layer takes categorical indices and return corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc07eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[8, 8],\n",
       "          [0, 5]],\n",
       " \n",
       "         [[1, 3],\n",
       "          [0, 8]],\n",
       " \n",
       "         [[1, 1],\n",
       "          [1, 7]]]),\n",
       " tensor([[[[ 0.0194, -0.8808,  0.9552],\n",
       "           [ 0.0194, -0.8808,  0.9552]],\n",
       " \n",
       "          [[ 0.1970, -1.1773, -0.0661],\n",
       "           [-1.4677, -0.8785, -2.0784]]],\n",
       " \n",
       " \n",
       "         [[[-0.3584, -1.5616, -0.3546],\n",
       "           [-1.4154, -1.0787, -0.7209]],\n",
       " \n",
       "          [[ 0.1970, -1.1773, -0.0661],\n",
       "           [ 0.0194, -0.8808,  0.9552]]],\n",
       " \n",
       " \n",
       "         [[[-0.3584, -1.5616, -0.3546],\n",
       "           [-0.3584, -1.5616, -0.3546]],\n",
       " \n",
       "          [[-0.3584, -1.5616, -0.3546],\n",
       "           [ 0.8403, -0.2635,  1.2805]]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomEmbeddingLayer(nn.Module):\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.weight = torch.randn(num_embeddings, embedding_dim)\n",
    "  def forward(self, x:torch.LongTensor):\n",
    "    '''\n",
    "    Argument\n",
    "      x: torch.LongTensor of arbitrary shape, where each element represent categorical index smaller than self.num_embeddings\n",
    "      \n",
    "    Return\n",
    "      out (torch.Tensor): torch.FloatTensor with [shape of x, self.embedding_dim]\n",
    "    \n",
    "    TODO: Complete this function using self.weight\n",
    "    '''\n",
    "    return self.weight[x]\n",
    "  \n",
    "custom_embedding_layer = CustomEmbeddingLayer(10, example_input_size)\n",
    "random_categorical_input = torch.randint(0,10, [3, 2, 2])\n",
    "random_categorical_input, custom_embedding_layer(random_categorical_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1b7d2",
   "metadata": {},
   "source": [
    "## Problem 3: Dataset (20 pts)\n",
    "- You have to declare a path for saving dataset\n",
    "- The dataset has vocabulary information\n",
    "    - For both pitch and duration, we added `'start'` and `'end'` token\n",
    "    - This helps a language model to start the generation or end the generation\n",
    "- You have to implment `__getitem__` of this dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013d05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\r\n"
     ]
    }
   ],
   "source": [
    "your_path = 'essen_folk/'\n",
    "'''\n",
    "You can download the dataset like this, but it will take too much time in Colab\n",
    "\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "'''\n",
    "# !pip install --upgrade gdown\n",
    "!gdown 1HMHgPifMFgRtIiLJsTb3ULqbxJx4xpQY # If it doesn't work, you have to upgrade gdown by !pip install --upgrade gdown\n",
    "\n",
    "# Following code will automatically unzip te dataset to essen_folk/\n",
    "!unzip -oq essen_converted.zip  # option: overwrite, quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba4a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading as the `.muspy.success` file is found.\n",
      "Skip extracting as the `.muspy.success` file is found.\n",
      "Skip conversion as the `.muspy.success` file is found.\n"
     ]
    }
   ],
   "source": [
    "class MelodyDataset:\n",
    "  def __init__(self, muspy_dataset, vocabs=None):\n",
    "    self.dataset = muspy_dataset\n",
    "    \n",
    "    if vocabs is None:\n",
    "      self.idx2pitch, self.idx2dur = self._get_vocab_info()\n",
    "      self.idx2pitch += ['start', 'end']\n",
    "      self.idx2dur += ['start', 'end']\n",
    "      self.pitch2idx = {x:i for i, x in enumerate(self.idx2pitch)}\n",
    "      self.dur2idx = {x:i for i, x in enumerate(self.idx2dur)}\n",
    "      \n",
    "    else:\n",
    "      self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    \n",
    "  def _get_vocab_info(self):\n",
    "    entire_pitch = []\n",
    "    entire_dur = []\n",
    "    for note_rep in self.dataset:\n",
    "      pitch_in_piece = note_rep[:, 1]\n",
    "      dur_in_piece = note_rep[:, 2]\n",
    "      entire_pitch += pitch_in_piece.tolist()\n",
    "      entire_dur += dur_in_piece.tolist()\n",
    "    return list(set(entire_pitch)), list(set(entire_dur))\n",
    "  \n",
    "  def get_vocabs(self):\n",
    "    return self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    This dataset class returns melody information as a tensor with shape of [num_notes, 2 (pitch, duration)].\n",
    "    \n",
    "    To train a melody language model, you have to provide a sequence of original note, and a sequence of next note for given original note.\n",
    "    In other word, melody[i+1] has to be the shifted_melody[i], so that melody[i]'s next note can be retrieved by shifted_melody[i]\n",
    "    (Remember, language model is trained to predict the next upcoming word)\n",
    "    \n",
    "    Also, to make genration easier, we usually add 'start' token at the beginning of sequence, and 'end' token at the end of the sequence.\n",
    "    With these tokens, we can make the model recognize where is the start and end of the sequence explicitly.\n",
    "    \n",
    "    You have to add these tokens to the note sequence at this step.\n",
    "    \n",
    "    Argument:\n",
    "      idx (int): Index of data sample in the dataset\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                 Has a shape of [1 (start_token) + num_notes, 2 (pitch, dur)]. \n",
    "                                 The first element of the sequence has to be the index for 'start' token for both pitch and duration.\n",
    "                                 The melody should not include 'end' token (Because we don't have to predict next note if we know that current note is 'end' token)\n",
    "      shifted_melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
    "                                         Has a shape of [num_notes + 1 (end_token), 2 (pitch, dur)]\n",
    "                                         The i'th note of shifted melody has to be the same with (i+1)'th note of melody\n",
    "                                         The shifted melody should not include 'start' token \n",
    "                                         (Because we never get a 'start' token after a note)\n",
    "                                         \n",
    "    ì´ ë°ì´í„° ì„¸íŠ¸ í´ëž˜ìŠ¤ëŠ” [num_notes, 2(í”¼ì¹˜, ì§€ì† ì‹œê°„)]ì˜ ëª¨ì–‘ì„ ê°€ì§„ í…ì„œë¡œ ë©œë¡œë”” ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    ë©œë¡œë”” ì–¸ì–´ ëª¨ë¸ì„ êµìœ¡í•˜ë ¤ë©´ ì›ë³¸ ë…¸íŠ¸ ì‹œí€€ìŠ¤ì™€ ì£¼ì–´ì§„ ì›ë³¸ ë…¸íŠ¸ì— ëŒ€í•œ ë‹¤ìŒ ë…¸íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    ì¦‰, ë©œë¡œë””[i+1]ëŠ” shifted_melody[i]ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëž˜ì•¼ ë©œë¡œë””[i]ì˜ ë‹¤ìŒ ìŒì„ shift_melody[i]ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "    (ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤.)\n",
    "    \n",
    "    ë˜í•œ ìƒì„±ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ ë³´í†µ ì‹œí€€ìŠ¤ì˜ ì‹œìž‘ ë¶€ë¶„ì— 'start' í† í°ì„ ì¶”ê°€í•˜ê³  ì‹œí€€ìŠ¤ ë ë¶€ë¶„ì— 'end' í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    ì´ëŸ¬í•œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ì‹œí€€ìŠ¤ì˜ ì‹œìž‘ê³¼ ëì´ ì–´ë””ì¸ì§€ ëª…ì‹œì ìœ¼ë¡œ ì¸ì‹í•˜ë„ë¡ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    ë‹¤ìŒì˜ ë‹¨ê³„ì—ì„œ ì´ëŸ¬í•œ í† í°ì„ ë…¸íŠ¸ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Argument:\n",
    "      idx(int): ë°ì´í„° ì§‘í•©ì˜ ë°ì´í„° ìƒ˜í”Œ ì¸ë±ìŠ¤ìž…ë‹ˆë‹¤.\n",
    "    \n",
    "    Returns:\n",
    "      melody (torch.LongTensor): [categorical_index_of_pitch, categorical_index_of_duration]ì˜ ìˆœì„œìž…ë‹ˆë‹¤.\n",
    "                                 ëª¨ì–‘ì€ [1(start_token) + num_notes, 2(í”¼ì¹˜, dur)]ìž…ë‹ˆë‹¤. \n",
    "                                 ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ìš”ì†ŒëŠ” í”¼ì¹˜ì™€ ì§€ì† ì‹œê°„ ëª¨ë‘ì— ëŒ€í•œ 'ì‹œìž‘' í† í°ì˜ ì¸ë±ìŠ¤ê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "                                 ë©œë¡œë””ì—ëŠ” 'ì¢…ë£Œ' í† í°ì´ í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤(í˜„ìž¬ ë…¸íŠ¸ê°€ 'ì¢…ë£Œ' í† í°ì¸ ê²½ìš° ë‹¤ìŒ ë…¸íŠ¸ë¥¼ ì˜ˆì¸¡í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤).\n",
    "      shifted_melody (torch.LongTensor): [categorical_index_of_pitch, categorical_index_of_duration]ì˜ ìˆœì„œìž…ë‹ˆë‹¤.\n",
    "                                         ëª¨ì–‘ì´ [num_notes + 1(end_token), 2(í”¼ì¹˜, dur)]ìž…ë‹ˆë‹¤.\n",
    "                                         ì´ë™ëœ ë©œë¡œë””ì˜ i' ìŒì€ (i+1)ì˜ ë©œë¡œë”” ìŒê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "                                         ì´ë™ëœ ë©œë¡œë””ì—ëŠ” 'ì‹œìž‘' í† í°ì´ í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. \n",
    "                                         (ë©”ëª¨ ë’¤ì— 'ì‹œìž‘' í† í°ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.)\n",
    "    TODO: Complete this function\n",
    "       now pitch dur  ?\n",
    "    [   0   72   24   64]\n",
    "    '''\n",
    "    note_representation = self.dataset[idx]\n",
    "    note_representation = note_representation[:,1:3].tolist()\n",
    "    melody_representation = [['start', 'start']] + note_representation\n",
    "    shifted_melody_representation = note_representation + [['end', 'end']]\n",
    "    \n",
    "    melody = []\n",
    "    for pitch_dur in melody_representation:\n",
    "      melody.append([self.pitch2idx[pitch_dur[0]], self.dur2idx[pitch_dur[1]]])\n",
    "  \n",
    "    shifted_melody = []\n",
    "    for pitch_dur in shifted_melody_representation:\n",
    "      shifted_melody.append([self.pitch2idx[pitch_dur[0]], self.dur2idx[pitch_dur[1]]])\n",
    "\n",
    "    return torch.LongTensor(melody), torch.LongTensor(shifted_melody)\n",
    "#     return torch.cat(melody, shifted_melody, dim=0)\n",
    "\n",
    "your_path = 'essen_folk/'\n",
    "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
    "essen.convert()\n",
    "\n",
    "essen_entire = essen.to_pytorch_dataset(representation='note')\n",
    "essen_split = essen.to_pytorch_dataset(representation='note', splits=(0.8, 0.1, 0.1), random_state=0)\n",
    "entire_set = MelodyDataset(essen_entire)\n",
    "\n",
    "train_set = MelodyDataset(essen_split['train'], vocabs=entire_set.get_vocabs())\n",
    "valid_set = MelodyDataset(essen_split['validation'], vocabs=entire_set.get_vocabs())\n",
    "test_set = MelodyDataset(essen_split['test'], vocabs=entire_set.get_vocabs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0372bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To check the MelodyDataset implementation\n",
    "'''\n",
    "\n",
    "assert len(train_set[0]) == 2, \"You have to return two variables at __getitem__\"\n",
    "assert train_set[0][0].shape == train_set[0][1].shape, \"Shape of Melody and Shifted melody has to be the same\"\n",
    "\n",
    "assert (train_set[0][0][0] == torch.LongTensor([38, 44])).all(), \"You have to add start token at the beginning of melody\"\n",
    "assert (train_set[0][1][-1] == torch.LongTensor([39, 45])).all(), \"You have to add end token at the end of melody\"\n",
    "\n",
    "assert (train_set[0][0][-1] == torch.LongTensor([12, 26])).all(), \"Last part of melody must not include the end token\"\n",
    "assert (train_set[0][1][0] == torch.LongTensor([24, 16])).all(),  \"First part of shifted melody must not include the start token\"\n",
    "\n",
    "assert (train_set[20][0][1:] == train_set[20][1][:-1]).all(), \"Check the melody shift\"\n",
    "\n",
    "print(\"Passed test cases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673c342",
   "metadata": {},
   "source": [
    "## PackSequence\n",
    "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
    "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
    "- In this problem, you will learn about how to handle sequences of different length as a batch\n",
    "\n",
    "- You can also refer [a video lecture](https://youtu.be/IQf1zu6jdCU) in Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83754613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [58, 2] at entry 0 and [25, 2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis cell will make error, because the length of each sample is different to each other\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:618\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    620\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [58, 2] at entry 0 and [25, 2] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "This cell will make error, because the length of each sample is different to each other\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8)\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58708cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To handle that problem, you have to make your collate function \n",
    "'''\n",
    "def your_collate_function(raw_batch):\n",
    "  '''\n",
    "  You can make your own function to handle the batch\n",
    "  '''\n",
    "  ret_batch = raw_batch[:-1]\n",
    "  ret_batch.append(raw_batch[0])\n",
    "#   return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
    "  return ret_batch\n",
    "\n",
    "batch_size = 8\n",
    "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
    "batch = your_collate_function(raw_batch)\n",
    "\n",
    "'''\n",
    "This is what the 'collate_fn' does in DataLoader\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
    "batch_by_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a209",
   "metadata": {},
   "source": [
    "#### Pad Sequence and Pack Sequence\n",
    "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
    "- `torch.nn.utils.rnn.pad_sequence`\n",
    "    - This function takes list of tensors with different length and return padded sequence\n",
    "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
    "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
    "    - In default, we use 0 for padding (zero padding)\n",
    "    - The result \n",
    "- `torch.nn.utils.rnn.pack_sequence`\n",
    "    - pad_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2a21e",
   "metadata": {},
   "source": [
    "Below cells show the example of pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24424475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  2.],\n",
       "        [ 1.,  6.,  3.],\n",
       "        [ 2.,  8.,  4.],\n",
       "        [ 0., 12.,  3.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  2.,  0.],\n",
       "        [ 0.,  3.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
    "short = torch.Tensor([0, 1, 2])\n",
    "long = torch.Tensor([3, 6, 8, 12, 1, 2, 3])\n",
    "middle = torch.Tensor([2, 3, 4, 3, 0])\n",
    "\n",
    "pad_sequence([short, long, middle], batch_first=False)  # T x N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99aec137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.,  0.,  0.,  0.],\n",
       "        [ 3.,  6.,  8., 12.,  1.,  2.,  3.],\n",
       "        [ 2.,  3.,  4.,  3.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default value of batch_first in pad_sequence is False.\n",
    "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
    "pad_sequence([short, long, middle], batch_first=True)  # N x T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b420b0",
   "metadata": {},
   "source": [
    "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
    "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
    "\n",
    "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
    "- RNN doesn't know whether it is padded data, or existing data\n",
    "- This makes computation slower\n",
    "\n",
    "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
    "\n",
    "To solve this issue, we use PackedSequence, by using `pack_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f553adab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 3.,  2.,  0.,  6.,  3.,  1.,  8.,  4.,  2., 12.,  3.,  1.,  0.,  2.,\n",
       "         3.]), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
    "packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c04cfe",
   "metadata": {},
   "source": [
    "`PackedSequence` has `data` and `batch_sizes`\n",
    "- `data` contains the flattened value of given batch\n",
    "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
    "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
    "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
    "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
    "    - `[1,2,0]` means that \n",
    "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
    "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
    "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
    "- `unsorted_indices` shows how the original sequences are sorted.\n",
    "    - `[2,0,1]` means that\n",
    "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc405b5",
   "metadata": {},
   "source": [
    "If you feed PackedSequence to RNN (or LSTM, GRU), it will return PackedSequence with same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a65589f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of output of RNN for PackedSequence: <class 'torch.nn.utils.rnn.PackedSequence'>\n",
      "Type of last_hidden of RNN for PackedSequence: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "rnn_layer = nn.GRU(1, 1)\n",
    "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
    "out, last_hidden = rnn_layer(packed_sequence)\n",
    "\n",
    "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
    "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df707e",
   "metadata": {},
   "source": [
    "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
    "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
    "- There are two ways to feed `PackedSequence` to these layers\n",
    "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
    "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e983f58",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not PackedSequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis will make error, because other layers cannot handle PackedSequence\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_linear_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtest_linear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_sequence\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not PackedSequence"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This will make error, because other layers cannot handle PackedSequence\n",
    "'''\n",
    "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
    "test_linear_layer(packed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1754e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded sequence generated from packed sequence (squeezed for printing): \n",
      " tensor([[ 0.,  3.,  2.],\n",
      "        [ 1.,  6.,  3.],\n",
      "        [ 2.,  8.,  4.],\n",
      "        [ 0., 12.,  3.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  2.,  0.],\n",
      "        [ 0.,  3.,  0.]])\n",
      "\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \n",
      " tensor([3, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "One way to to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
    "'''\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
    "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
    "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7871b5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of feeding padded_sequence to a linear layer: tensor([[[-0.5267,  0.1322],\n",
      "         [-0.4853, -0.3779],\n",
      "         [-0.4991, -0.2079]],\n",
      "\n",
      "        [[-0.5129, -0.0378],\n",
      "         [-0.4440, -0.8879],\n",
      "         [-0.4853, -0.3779]],\n",
      "\n",
      "        [[-0.4991, -0.2079],\n",
      "         [-0.4164, -1.2280],\n",
      "         [-0.4715, -0.5479]],\n",
      "\n",
      "        [[-0.5267,  0.1322],\n",
      "         [-0.3613, -1.9080],\n",
      "         [-0.4853, -0.3779]],\n",
      "\n",
      "        [[-0.5267,  0.1322],\n",
      "         [-0.5129, -0.0378],\n",
      "         [-0.5267,  0.1322]],\n",
      "\n",
      "        [[-0.5267,  0.1322],\n",
      "         [-0.4991, -0.2079],\n",
      "         [-0.5267,  0.1322]],\n",
      "\n",
      "        [[-0.5267,  0.1322],\n",
      "         [-0.4853, -0.3779],\n",
      "         [-0.5267,  0.1322]]], grad_fn=<ViewBackward0>)\n",
      "Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now you can feed padded sequence to linear layer.\n",
    "'''\n",
    "\n",
    "linear_output = test_linear_layer(padded_sequence)\n",
    "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
    "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71d104d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.4853, -0.3779],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.5267,  0.1322],\n",
       "        [-0.4440, -0.8879],\n",
       "        [-0.4853, -0.3779],\n",
       "        [-0.5129, -0.0378],\n",
       "        [-0.4164, -1.2280],\n",
       "        [-0.4715, -0.5479],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.3613, -1.9080],\n",
       "        [-0.4853, -0.3779],\n",
       "        [-0.5129, -0.0378],\n",
       "        [-0.5267,  0.1322],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.4853, -0.3779]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
    "'''\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
    "re_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a869f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.4853, -0.3779],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.5267,  0.1322],\n",
       "        [-0.4440, -0.8879],\n",
       "        [-0.4853, -0.3779],\n",
       "        [-0.5129, -0.0378],\n",
       "        [-0.4164, -1.2280],\n",
       "        [-0.4715, -0.5479],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.3613, -1.9080],\n",
       "        [-0.4853, -0.3779],\n",
       "        [-0.5129, -0.0378],\n",
       "        [-0.5267,  0.1322],\n",
       "        [-0.4991, -0.2079],\n",
       "        [-0.4853, -0.3779]], grad_fn=<AddmmBackward0>), batch_sizes=tensor([3, 3, 3, 2, 2, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Another way to do it is using PackedSequence.data\n",
    "'''\n",
    "\n",
    "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
    "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
    "packed_sequence_after_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef97f8e",
   "metadata": {},
   "source": [
    "## Problem 4: Implement pack_collate(), (20 pts)\n",
    "- Implement a collate function that returns PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "518ed3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([341, 2]) torch.Size([341, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         [19, 10],\n",
       "         [24, 16],\n",
       "         [27, 14],\n",
       "         [24, 10],\n",
       "         [34, 10],\n",
       "         [24, 10],\n",
       "         [26, 16],\n",
       "         [28, 16],\n",
       "         [17,  5],\n",
       "         [24, 16],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [31, 10],\n",
       "         [26, 22],\n",
       "         [21, 16],\n",
       "         [16, 16],\n",
       "         [16,  5],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [26, 10],\n",
       "         [29, 16],\n",
       "         [31, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [20, 10],\n",
       "         [29, 10],\n",
       "         [34, 10],\n",
       "         [26, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [14,  5],\n",
       "         [22, 10],\n",
       "         [22, 10],\n",
       "         [24, 26],\n",
       "         [24, 22],\n",
       "         [28,  5],\n",
       "         [21, 16],\n",
       "         [21,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [15, 10],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [26,  5],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [17, 16],\n",
       "         [26,  5],\n",
       "         [24, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [27, 14],\n",
       "         [24,  5],\n",
       "         [29, 10],\n",
       "         [21, 10],\n",
       "         [26, 16],\n",
       "         [26, 10],\n",
       "         [28,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [34,  5],\n",
       "         [24, 10],\n",
       "         [19, 10],\n",
       "         [28, 10],\n",
       "         [26, 33],\n",
       "         [19, 26],\n",
       "         [22, 10],\n",
       "         [19, 10],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [23, 26],\n",
       "         [24, 14],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [17, 26],\n",
       "         [29, 22],\n",
       "         [19, 10],\n",
       "         [14, 16],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [14, 26],\n",
       "         [26, 10],\n",
       "         [16, 16],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [15, 10],\n",
       "         [17, 16],\n",
       "         [34, 10],\n",
       "         [24, 16],\n",
       "         [31, 10],\n",
       "         [19,  5],\n",
       "         [19,  5],\n",
       "         [22, 10],\n",
       "         [17, 16],\n",
       "         [17, 10],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [27, 14],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [28, 10],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [14, 22],\n",
       "         [24, 10],\n",
       "         [29,  5],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [26,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [15, 10],\n",
       "         [27, 10],\n",
       "         [24, 10],\n",
       "         [34, 14],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23, 10],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 26],\n",
       "         [27, 10],\n",
       "         [17,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [21, 10],\n",
       "         [19,  5],\n",
       "         [19, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [19,  5],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [16, 10],\n",
       "         [23,  5],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [22, 16],\n",
       "         [24, 10],\n",
       "         [34,  5],\n",
       "         [21, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [26, 10],\n",
       "         [29, 14],\n",
       "         [20, 10],\n",
       "         [21,  5],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [19,  5],\n",
       "         [24, 10],\n",
       "         [26,  5],\n",
       "         [20, 16],\n",
       "         [19,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [14, 26],\n",
       "         [16, 26],\n",
       "         [21, 10],\n",
       "         [24, 10],\n",
       "         [17, 10],\n",
       "         [17, 26],\n",
       "         [31,  5],\n",
       "         [14, 26],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [29, 22],\n",
       "         [19,  5],\n",
       "         [26, 10],\n",
       "         [12, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [29, 10],\n",
       "         [10, 16],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [24, 16],\n",
       "         [27, 10],\n",
       "         [21, 10],\n",
       "         [14, 26],\n",
       "         [22, 10],\n",
       "         [27,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [24, 26],\n",
       "         [22, 10],\n",
       "         [21,  5],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [27, 10],\n",
       "         [19,  5],\n",
       "         [26,  5],\n",
       "         [19,  5],\n",
       "         [22, 16],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19,  5],\n",
       "         [20, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [15, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [14, 10],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [12, 10],\n",
       "         [14,  5],\n",
       "         [14, 10],\n",
       "         [14, 10],\n",
       "         [10, 26],\n",
       "         [12, 16],\n",
       "         [26, 10],\n",
       "         [12, 26],\n",
       "         [26, 10],\n",
       "         [17, 10],\n",
       "         [31, 10],\n",
       "         [15, 16],\n",
       "         [26,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19, 26],\n",
       "         [21, 26],\n",
       "         [24, 26],\n",
       "         [26, 10],\n",
       "         [19, 16],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [26,  5],\n",
       "         [17, 22],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [17, 10],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [19,  5],\n",
       "         [14, 10],\n",
       "         [17, 10],\n",
       "         [12, 26],\n",
       "         [14, 10],\n",
       "         [19, 10],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17,  5],\n",
       "         [14, 25],\n",
       "         [19, 10],\n",
       "         [17,  5],\n",
       "         [16,  5],\n",
       "         [14, 10],\n",
       "         [14,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [28,  5],\n",
       "         [26, 33],\n",
       "         [24, 14],\n",
       "         [21,  5],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [14, 22],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 10],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [14, 26]]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "         8, 6, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[19, 10],\n",
       "         [24, 16],\n",
       "         [27, 14],\n",
       "         [24, 10],\n",
       "         [34, 10],\n",
       "         [24, 10],\n",
       "         [26, 16],\n",
       "         [28, 16],\n",
       "         [17,  5],\n",
       "         [24, 16],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [31, 10],\n",
       "         [26, 22],\n",
       "         [21, 16],\n",
       "         [16, 16],\n",
       "         [16,  5],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [26, 10],\n",
       "         [29, 16],\n",
       "         [31, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [20, 10],\n",
       "         [29, 10],\n",
       "         [34, 10],\n",
       "         [26, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [14,  5],\n",
       "         [22, 10],\n",
       "         [22, 10],\n",
       "         [24, 26],\n",
       "         [24, 22],\n",
       "         [28,  5],\n",
       "         [21, 16],\n",
       "         [21,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [15, 10],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [26,  5],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [17, 16],\n",
       "         [26,  5],\n",
       "         [24, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [27, 14],\n",
       "         [24,  5],\n",
       "         [29, 10],\n",
       "         [21, 10],\n",
       "         [26, 16],\n",
       "         [26, 10],\n",
       "         [28,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [34,  5],\n",
       "         [24, 10],\n",
       "         [19, 10],\n",
       "         [28, 10],\n",
       "         [26, 33],\n",
       "         [19, 26],\n",
       "         [22, 10],\n",
       "         [19, 10],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [23, 26],\n",
       "         [24, 14],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [17, 26],\n",
       "         [29, 22],\n",
       "         [19, 10],\n",
       "         [14, 16],\n",
       "         [28, 16],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [20,  5],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [14, 26],\n",
       "         [26, 10],\n",
       "         [16, 16],\n",
       "         [24, 10],\n",
       "         [29, 10],\n",
       "         [22, 10],\n",
       "         [14, 10],\n",
       "         [24, 16],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [24, 16],\n",
       "         [15, 10],\n",
       "         [17, 16],\n",
       "         [34, 10],\n",
       "         [24, 16],\n",
       "         [31, 10],\n",
       "         [19,  5],\n",
       "         [19,  5],\n",
       "         [22, 10],\n",
       "         [17, 16],\n",
       "         [17, 10],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [27, 14],\n",
       "         [29, 10],\n",
       "         [29, 16],\n",
       "         [28, 10],\n",
       "         [26, 10],\n",
       "         [23, 16],\n",
       "         [14, 22],\n",
       "         [24, 10],\n",
       "         [29,  5],\n",
       "         [26, 10],\n",
       "         [31, 10],\n",
       "         [26,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19,  5],\n",
       "         [15, 10],\n",
       "         [27, 10],\n",
       "         [24, 10],\n",
       "         [34, 14],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [23, 10],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 26],\n",
       "         [27, 10],\n",
       "         [17,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [21, 10],\n",
       "         [19,  5],\n",
       "         [19, 10],\n",
       "         [29, 10],\n",
       "         [24, 10],\n",
       "         [19,  5],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [16, 10],\n",
       "         [23,  5],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [22, 16],\n",
       "         [24, 10],\n",
       "         [34,  5],\n",
       "         [21, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [26, 10],\n",
       "         [29, 14],\n",
       "         [20, 10],\n",
       "         [21,  5],\n",
       "         [31,  5],\n",
       "         [21, 10],\n",
       "         [16, 10],\n",
       "         [19,  5],\n",
       "         [24, 10],\n",
       "         [26,  5],\n",
       "         [20, 16],\n",
       "         [19,  5],\n",
       "         [29,  5],\n",
       "         [19, 10],\n",
       "         [14, 26],\n",
       "         [16, 26],\n",
       "         [21, 10],\n",
       "         [24, 10],\n",
       "         [17, 10],\n",
       "         [17, 26],\n",
       "         [31,  5],\n",
       "         [14, 26],\n",
       "         [39, 45],\n",
       "         [39, 45],\n",
       "         [17, 10],\n",
       "         [29, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [29, 22],\n",
       "         [39, 45],\n",
       "         [19,  5],\n",
       "         [26, 10],\n",
       "         [12, 10],\n",
       "         [19,  5],\n",
       "         [39, 45],\n",
       "         [21,  5],\n",
       "         [29, 10],\n",
       "         [10, 16],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [24, 16],\n",
       "         [27, 10],\n",
       "         [21, 10],\n",
       "         [14, 26],\n",
       "         [22, 10],\n",
       "         [27,  5],\n",
       "         [19, 10],\n",
       "         [26, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [26, 10],\n",
       "         [24, 26],\n",
       "         [22, 10],\n",
       "         [21,  5],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [27, 10],\n",
       "         [19,  5],\n",
       "         [26,  5],\n",
       "         [19,  5],\n",
       "         [22, 16],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19,  5],\n",
       "         [20, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [24, 10],\n",
       "         [15, 10],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [17, 10],\n",
       "         [14, 10],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [17, 14],\n",
       "         [17, 10],\n",
       "         [15, 10],\n",
       "         [12, 10],\n",
       "         [14,  5],\n",
       "         [14, 10],\n",
       "         [14, 10],\n",
       "         [10, 26],\n",
       "         [12, 16],\n",
       "         [26, 10],\n",
       "         [12, 26],\n",
       "         [39, 45],\n",
       "         [39, 45],\n",
       "         [26, 10],\n",
       "         [17, 10],\n",
       "         [31, 10],\n",
       "         [15, 16],\n",
       "         [26,  5],\n",
       "         [17, 10],\n",
       "         [24,  5],\n",
       "         [19, 26],\n",
       "         [21, 26],\n",
       "         [24, 26],\n",
       "         [26, 10],\n",
       "         [19, 16],\n",
       "         [26, 10],\n",
       "         [21, 10],\n",
       "         [31, 10],\n",
       "         [19, 10],\n",
       "         [26,  5],\n",
       "         [17, 22],\n",
       "         [24,  5],\n",
       "         [21, 10],\n",
       "         [21,  5],\n",
       "         [19, 10],\n",
       "         [24,  5],\n",
       "         [17, 10],\n",
       "         [21,  5],\n",
       "         [15, 10],\n",
       "         [19,  5],\n",
       "         [14, 10],\n",
       "         [17, 10],\n",
       "         [12, 26],\n",
       "         [14, 10],\n",
       "         [39, 45],\n",
       "         [19, 10],\n",
       "         [19, 10],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17,  5],\n",
       "         [14, 25],\n",
       "         [19, 10],\n",
       "         [17,  5],\n",
       "         [16,  5],\n",
       "         [14, 10],\n",
       "         [14,  5],\n",
       "         [14,  5],\n",
       "         [19, 10],\n",
       "         [21,  5],\n",
       "         [28,  5],\n",
       "         [26, 33],\n",
       "         [24, 14],\n",
       "         [21,  5],\n",
       "         [24, 10],\n",
       "         [21,  5],\n",
       "         [19,  5],\n",
       "         [17, 10],\n",
       "         [14, 22],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 16],\n",
       "         [19, 10],\n",
       "         [21, 16],\n",
       "         [26, 10],\n",
       "         [24, 10],\n",
       "         [21, 10],\n",
       "         [17, 10],\n",
       "         [19,  5],\n",
       "         [21,  5],\n",
       "         [17, 10],\n",
       "         [14, 26],\n",
       "         [39, 45]]), batch_sizes=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "         8, 6, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "def pack_collate(raw_batch:list):\n",
    "  '''\n",
    "  This function takes a list of data, and returns two PackedSequences\n",
    "  \n",
    "  Argument\n",
    "    raw_batch: A list of MelodyDataset[idx]. Each item in the list is a tuple of (melody, shifted_melody)\n",
    "               melody and shifted_melody has a shape of [num_notes (+1 if you don't consider \"start\" and \"end\" token as note), 2]\n",
    "  Returns\n",
    "    packed_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "    packed_shifted_melody (torch.nn.utils.rnn.PackedSequence)\n",
    "\n",
    "  TODO: Complete this function\n",
    "  '''  \n",
    "  melody = [x[0] for x in raw_batch]\n",
    "  shifted_melody = [x[1] for x in raw_batch]\n",
    "  \n",
    "  melody.sort(key=lambda x:len(x), reverse=True)\n",
    "  shifted_melody.sort(key=lambda x:len(x), reverse=True)\n",
    "  \n",
    "  packed_melody = pack_sequence(melody)\n",
    "  packed_shifted_melody = pack_sequence(shifted_melody)\n",
    "  return packed_melody, packed_shifted_melody\n",
    "\n",
    "raw_batch = [train_set[i] for i in range(batch_size)]\n",
    "packed_melody, packed_shifted_melody = pack_collate(raw_batch)\n",
    "print(packed_melody.data.shape, packed_shifted_melody.data.shape)\n",
    "packed_melody, packed_shifted_melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1e46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all the test cases\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test whether you have implemented pack_collate correctly\n",
    "'''\n",
    "\n",
    "assert isinstance(packed_melody, PackedSequence)\n",
    "assert isinstance(packed_shifted_melody, PackedSequence)\n",
    "\n",
    "assert packed_melody.data.shape==packed_shifted_melody.data.shape\n",
    "\n",
    "print(\"Passed all the test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d182eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[38, 44],\n",
       "         [38, 44],\n",
       "         [38, 44],\n",
       "         ...,\n",
       "         [26, 16],\n",
       "         [28, 10],\n",
       "         [31, 26]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 61, 61, 61, 61, 61, 61, 60, 58, 57, 56, 55, 53, 52, 51, 50, 48, 45,\n",
       "         43, 43, 41, 41, 39, 36, 34, 34, 33, 32, 32, 31, 31, 30, 29, 29, 29, 28,\n",
       "         27, 27, 26, 26, 26, 24, 21, 19, 19, 18, 15, 15, 15, 15, 15, 15, 15, 13,\n",
       "         13, 13, 12, 12, 11, 10, 10,  9,  9,  9,  9,  7,  7,  7,  6,  4,  4,  4,\n",
       "          4,  4,  4,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[23, 14],\n",
       "         [30,  5],\n",
       "         [19, 10],\n",
       "         ...,\n",
       "         [28, 10],\n",
       "         [31, 26],\n",
       "         [39, 45]]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63,\n",
       "         63, 61, 61, 61, 61, 61, 61, 60, 58, 57, 56, 55, 53, 52, 51, 50, 48, 45,\n",
       "         43, 43, 41, 41, 39, 36, 34, 34, 33, 32, 32, 31, 31, 30, 29, 29, 29, 28,\n",
       "         27, 27, 26, 26, 26, 24, 21, 19, 19, 18, 15, 15, 15, 15, 15, 15, 15, 13,\n",
       "         13, 13, 12, 12, 11, 10, 10,  9,  9,  9,  9,  7,  7,  7,  6,  4,  4,  4,\n",
       "          4,  4,  4,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263352",
   "metadata": {},
   "source": [
    "## Problem 5: Define Melody Language Model (25 pts)\n",
    "- In this problem, you have to define a Language Model for model\n",
    "    - It is almost same as an ordinary language model for natural language processing\n",
    "    - The key difference is that the melody language model has to predict pitch **and** duration\n",
    "- Complete the model step-by-step\n",
    "    - Complete each function and test the function with the cells below\n",
    "    - `get_concat_embedding()` makes concatenated embedding for each note given pitch and duration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "893efba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[0.0241, 0.0254, 0.0269,  ..., 0.0262, 0.0252, 0.0234],\n",
       "         [0.0241, 0.0254, 0.0269,  ..., 0.0262, 0.0252, 0.0234],\n",
       "         [0.0241, 0.0254, 0.0269,  ..., 0.0262, 0.0252, 0.0234],\n",
       "         ...,\n",
       "         [0.0218, 0.0233, 0.0298,  ..., 0.0227, 0.0246, 0.0257],\n",
       "         [0.0222, 0.0233, 0.0303,  ..., 0.0228, 0.0251, 0.0257],\n",
       "         [0.0223, 0.0235, 0.0306,  ..., 0.0231, 0.0254, 0.0256]],\n",
       "        grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63,\n",
       "         62, 61, 60, 59, 59, 59, 59, 59, 59, 59, 57, 56, 53, 52, 51, 51, 50, 48,\n",
       "         46, 45, 42, 41, 40, 40, 39, 39, 39, 39, 36, 36, 35, 35, 34, 32, 31, 29,\n",
       "         28, 25, 25, 24, 23, 23, 22, 21, 21, 21, 20, 18, 17, 17, 17, 16, 16, 16,\n",
       "         15, 15, 14, 14, 13, 12, 12, 12, 12, 11, 11, 11, 10,  9,  9,  8,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  3,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None),\n",
       " PackedSequence(data=tensor([[0.0250, 0.0203, 0.0238,  ..., 0.0209, 0.0221, 0.0212],\n",
       "         [0.0250, 0.0203, 0.0238,  ..., 0.0209, 0.0221, 0.0212],\n",
       "         [0.0250, 0.0203, 0.0238,  ..., 0.0209, 0.0221, 0.0212],\n",
       "         ...,\n",
       "         [0.0258, 0.0192, 0.0232,  ..., 0.0198, 0.0251, 0.0191],\n",
       "         [0.0260, 0.0193, 0.0234,  ..., 0.0199, 0.0247, 0.0195],\n",
       "         [0.0264, 0.0192, 0.0234,  ..., 0.0201, 0.0243, 0.0195]],\n",
       "        grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63,\n",
       "         62, 61, 60, 59, 59, 59, 59, 59, 59, 59, 57, 56, 53, 52, 51, 51, 50, 48,\n",
       "         46, 45, 42, 41, 40, 40, 39, 39, 39, 39, 36, 36, 35, 35, 34, 32, 31, 29,\n",
       "         28, 25, 25, 24, 23, 23, 22, 21, 21, 21, 20, 18, 17, 17, 17, 16, 16, 16,\n",
       "         15, 15, 14, 14, 13, 12, 12, 12, 12, 11, 11, 11, 10,  9,  9,  8,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  3,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "class MelodyLanguageModel(nn.Module):\n",
    "  def __init__(self, hidden_size, embed_size, vocabs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embed_size = embed_size\n",
    "    self.num_pitch = len(self.idx2pitch)\n",
    "    self.num_dur = len(self.idx2dur)\n",
    "    self.num_layers = 3\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    TODO: Declare four modules. Please follow the name strictly.\n",
    "      1) self.pitch_embedder: nn.Embedding layer that embed pitch category index to a vector with size of 'embed_size'\n",
    "      2) self.dur_embedder = nn.Embedding layer that embed duration category index to a vector with size of 'embed_size'\n",
    "      3) self.rnn = nn.GRU layer that takes concatenated_embedding \n",
    "                    and has a hidden size of 'hidden_size', num_layers of self.num_layers, and batch_first=True\n",
    "      4) self.final_layer = nn.Linear layer that takes self.rnn's output and convert it \n",
    "                            to logits (that can be used as input of softmax) of pitch + duration\n",
    "   \n",
    "   TODO: ëª¨ë“ˆ 4ê°œë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤. ì´ë¦„ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì£¼ì„¸ìš”.\n",
    "      1) self.pitch_embedder: 'embed_size' í¬ê¸°ì˜ ë²¡í„°ì— í”¼ì¹˜ ë²”ì£¼ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•˜ëŠ” nn.Embedding layer.\n",
    "      2) self.dur_embedder = duration category indexë¥¼ 'embed_size' í¬ê¸°ì˜ ë²¡í„°ì— í¬í•¨í•˜ëŠ” nn.Embedding layer.\n",
    "      3) self.rnn = concatated_sizeë¥¼ ì‚¬ìš©í•˜ê³  ìˆ¨ê²¨ì§„ í¬ê¸°ê°€ 'hidden_size'ì¸ GRU ê³„ì¸µ, num_layers of self.num_layers,  batch_first=True\n",
    "      4) self.final_layer = [nn.Linear layer] self.rnnì˜ ì¶œë ¥ì„ ê°€ì ¸ì™€ì„œ \n",
    "                            'pitch + duration'ì˜ ë¡œì§“(ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ìž…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìžˆìŒ)ìœ¼ë¡œ ë³€í™˜\n",
    "      self.rnn = GRU(80, 64, num_layers=3, batch_first=True)\n",
    "    '''    \n",
    "    \n",
    "    self.pitch_embedder = nn.Embedding(self.num_pitch, self.embed_size)\n",
    "    self.dur_embedder = nn.Embedding(self.num_dur, self.embed_size)\n",
    "    self.rnn =  nn.GRU(2*self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "    self.final_layer = nn.Linear(self.hidden_size, self.num_pitch + self.num_dur)\n",
    "    \n",
    "  def get_concat_embedding(self, input_seq):\n",
    "    '''\n",
    "    This function returns concatenated pitch embedding and duration embedding for a given input seq\n",
    "    \n",
    "    Arguments:\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or [num_timesteps (num_notes), 2]\n",
    "                 \n",
    "                 ë²¡í„° (pitch_idx, dur_idx)ì˜ ì‹œí€€ìŠ¤ë¡œ í‘œí˜„ëœ ë©œë¡œë””ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°°ì¹˜. \n",
    "                 Shapeì€ [ë°°ì¹˜ ìƒ˜í”Œ ìˆ˜, íƒ€ìž„ìŠ¤í…ì˜ ìˆ˜ (==ìŒí‘œì˜ ìˆ˜), 2 (ìŒê³ , ê¸¸ì´)] í˜¹ì€ [íƒ€ìž„ìŠ¤í…ì˜ ìˆ˜ (num_notes), 2]\n",
    "    Return:\n",
    "      concat_embedding: A batch of sequence of concatenated embedding of pitch embedding and duration embedding.\n",
    "                        Has a shape of [num_batch, num_timesteps (num_notes), embedding_size * 2]\n",
    "                        Each vector of time t is [pitch_embedding ; duration_embedding] (concatenation)\n",
    "                        \n",
    "                        pitch embedding is the output of an nn.Embedding layer of given note pitch index\n",
    "                        duration embedding is the output of an nn.Embedding layer of given note duration index\n",
    "    \n",
    "    \n",
    "    TODO: Complete this function using self.pitch_embedder and self.dur_embedder\n",
    "    You can use torch.cat to concatenate two tensors or vectors\n",
    "    '''\n",
    "#     print(input_seq)\n",
    "    input_pitch = input_seq[..., 0]\n",
    "    input_dur = input_seq[..., 1]\n",
    "    return torch.cat([self.pitch_embedder(input_pitch), self.dur_embedder(input_dur)], dim=-1)\n",
    "  \n",
    "  \n",
    "  def initialize_rnn(self, batch_size: int) -> torch.Tensor :\n",
    "    '''\n",
    "    This function returns initial hidden state for self.rnn for given batch_size\n",
    "    \n",
    "    Argument\n",
    "      batch_size (int): \n",
    "      \n",
    "    Return\n",
    "      initial_hidden_state (torch.Tensor):\n",
    "    '''\n",
    "    \n",
    "    return torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "  \n",
    "    \n",
    "  \n",
    "  def forward(self, input_seq:torch.LongTensor):\n",
    "    '''\n",
    "    Forward propgation of Melody Language Model.\n",
    "    \n",
    "    Argument\n",
    "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx). \n",
    "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or can be a PackedSequence\n",
    "                 ë²¡í„° (pitch_idx, dur_idx)ì˜ ì‹œí€€ìŠ¤ë¡œ í‘œí˜„ëœ ë©œë¡œë””ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°°ì¹˜. \n",
    "                 Shapeì€ [ë°°ì¹˜ ìƒ˜í”Œ ìˆ˜, íƒ€ìž„ìŠ¤í…ì˜ ìˆ˜ (==ìŒí‘œì˜ ìˆ˜), 2 (ìŒê³ , ê¸¸ì´)] í˜¹ì€ PackedSequence.\n",
    "    \n",
    "    Output\n",
    "      pitch_dist: Probability distribution of pitch of next upcoming note for each timestep 't'.\n",
    "                  Has a shape of [num_batch, numtimesteps, self.num_pitch]\n",
    "                ë§¤ íƒ€ìž„ ìŠ¤í… tì— ëŒ€í•´, ê·¸ ë‹¤ìŒì— ë“±ìž¥í•  ìŒí‘œ ìŒê³ ì˜ í™•ë¥  ë¶„í¬\n",
    "      dur_dist: Probability distribution of duration of next upcoming note for each timestep 't'.\n",
    "                Has a shape of [num_batch, numtimesteps, self.num_dur]\n",
    "                ë§¤ íƒ€ìž„ ìŠ¤í… tì— ëŒ€í•´, ê·¸ ë‹¤ìŒì— ë“±ìž¥í•  ìŒí‘œ ê¸¸ì´ì˜ í™•ë¥  ë¶„í¬\n",
    "      \n",
    "    '''\n",
    "      \n",
    "  \n",
    "    '''\n",
    "    TODO: Complete this function. You have to handle both cases: input_seq as ordinary Tensor / input_seq as PackedSequence\n",
    "    If the input_seq is PackedSequence, return PackedSequence\n",
    "    \n",
    "    \n",
    "    input_seq â†’ self.get_concat_embedding â†’ self.rnn â†’ self.final_layer â†’ torch.softmax for [pitch, duration]\n",
    "    \n",
    "    Follow the instruction\n",
    "    '''\n",
    "\n",
    "    if isinstance(input_seq, torch.Tensor): # If input is an ordinary tensor\n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      x = self.get_concat_embedding(input_seq)\n",
    "      \n",
    "      # 2. Put concatenated_embeddings to self.rnn.\n",
    "      # Remember: RNN, GRU, LSTM returns two outputs\n",
    "      h_0 = self.initialize_rnn(batch_size=x.size(0))\n",
    "      out, _ = self.rnn(x, h_0)\n",
    "      \n",
    "      # 3. Put rnn's output with a shape of [num_batch, num_timestep, hidden_size] to self.final_layer\n",
    "      logits = self.final_layer(out)\n",
    "      \n",
    "      # 4. Convert logits (output of self.final_layer) to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      # 4. ë¡œì§“(self.final_layer ì¶œë ¥)ì„ í”¼ì¹˜ í™•ë¥  ë° ì§€ì† í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "      # ì¡°ì‹¬í•˜ì„¸ìš”! ì†Œí”„íŠ¸ë§¥ìŠ¤ í”¼ì¹˜ì™€ ì§€ì†ì‹œê°„ì„ ë”°ë¡œ êµ¬í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.\n",
    "      # ì™œëƒí•˜ë©´ í™•ë¥  ë¶„í¬ì—ì„œ í”¼ì¹˜ 1ê°œì™€ ì§€ì† ì‹œê°„ 1ê°œë¥¼ ì„ íƒí•´ì•¼ í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n",
    "      \n",
    "      pitch_out = logits[..., :self.num_pitch]\n",
    "      dur_out = logits[..., self.num_pitch:]\n",
    "      pitch_out = torch.softmax(pitch_out, dim=-1)\n",
    "      dur_out = torch.softmax(dur_out, dim=-1)\n",
    "\n",
    "      return pitch_out, dur_out\n",
    "    \n",
    "    elif isinstance(input_seq, PackedSequence):      \n",
    "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
    "      # To get concatenated_embeddings, You have to either pad_packed_sequence(input_seq, batch_first=True)\n",
    "      # Or use input_seq.data, \n",
    "      # and then make new PackedSequence using concatenated_embeddings as data, and copy batch_lengths, sorted_indices, unsorted_indices.\n",
    "      '''\n",
    "      1) pack -> pad, using pad_packed_sequence(input_seq, batch_first=True)\n",
    "      2) ìƒˆë¡œìš´ PackedSequence(embedding_data, batch_lengths, sorted_indices, unsorted_indices)\n",
    "      '''\n",
    "#       pad_seq, pad_seq_len = pad_packed_sequence(input_seq, batch_first=True)\n",
    "#       embed = self.get_concat_embedding(pad_seq)\n",
    "      embed = self.get_concat_embedding(input_seq.data)\n",
    "      embed_pack_seq = PackedSequence(embed, batch_sizes=input_seq.batch_sizes,\n",
    "                                      sorted_indices=input_seq.sorted_indices, unsorted_indices=input_seq.unsorted_indices)\n",
    "    \n",
    "      # 2. Put concatenated embedding to self.rnn\n",
    "      out, _ = self.rnn(embed_pack_seq)\n",
    "  \n",
    "      # 3. Put rnn output to self.final_layer to get probability logit for pitch and duration\n",
    "      # Again, rnn's output is PackedSequence so you have to handle it\n",
    "      # 3. rnn ì¶œë ¥ì„ self.final_layerì— ë„£ì–´ í”¼ì¹˜ ë° ì§€ì† ì‹œê°„ì— ëŒ€í•œ í™•ë¥  ë¡œì§“(probability logit)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "      # ë‹¤ì‹œ ë§í•˜ì§€ë§Œ rnnì˜ ì¶œë ¥ì€ PackedSequenceì´ë¯€ë¡œ ì‚¬ìš©ìžê°€ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "      '''\n",
    "      1) out(PackedSequence) -> unpack_out(pad)\n",
    "      2) logit = self.final_layer(unpack_out)\n",
    "      '''\n",
    "      unpacked_out, unpacked_out_len = pad_packed_sequence(out, batch_first=True)\n",
    "      logits = self.final_layer(unpacked_out)\n",
    "      \n",
    "      # 4. Convert logits to pitch probability and duration probability\n",
    "      # Caution! You have to get separately softmax-ed pitch and duration\n",
    "      # Because you have to pick one pitch and one duration from the probability distribution\n",
    "      # 4. ë¡œì§“ë“¤ì„ í”¼ì¹˜ í™•ë¥ ê³¼ ì§€ì† í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "      # ì¡°ì‹¬í•˜ì„¸ìš”! ì†Œí”„íŠ¸ë§¥ìŠ¤ í”¼ì¹˜ì™€ ì§€ì†ì‹œê°„ì„ ë”°ë¡œ êµ¬í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.\n",
    "      # í™•ë¥  ë¶„í¬ì—ì„œ í”¼ì¹˜ 1ê°œì™€ ì§€ì† ì‹œê°„ 1ê°œë¥¼ ì„ íƒí•´ì•¼ í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n",
    "      \n",
    "      pitch_out = logits[..., :self.num_pitch]\n",
    "      dur_out = logits[..., self.num_pitch:]\n",
    "      pitch_out = torch.softmax(pitch_out, dim=-1)\n",
    "      dur_out = torch.softmax(dur_out, dim=-1)\n",
    "\n",
    "      # Return output as PackedSequence \n",
    "      packed_pitch_out = pack_padded_sequence(pitch_out, unpacked_out_len, batch_first=True)\n",
    "      packed_dur_out = pack_padded_sequence(dur_out, unpacked_out_len, batch_first=True)\n",
    "      \n",
    "      return packed_pitch_out, packed_dur_out\n",
    "\n",
    "    else:\n",
    "      print(f\"Unrecognized input type: {type(input_seq)}\")\n",
    "  \n",
    "\n",
    "hidden_size = 64\n",
    "embed_size = 40\n",
    "    \n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "108a8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your concart_embedding: \n",
      "tensor([[[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [-0.1169,  0.3728, -1.0211,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         [-1.2959, -0.8071,  2.1192,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         ...,\n",
      "         [-0.2746,  0.4432,  0.9591,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         [ 0.7014, -0.5556, -0.3817,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         [ 0.5842,  1.0504,  1.2856,  ...,  0.0507, -0.7415, -0.2291]],\n",
      "\n",
      "        [[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [-0.1169,  0.3728, -1.0211,  ...,  0.4952, -0.9355, -0.6429],\n",
      "         [ 2.1245,  1.3875,  0.8115,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         ...,\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524]],\n",
      "\n",
      "        [[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [ 0.3763,  1.1280,  0.0219,  ...,  0.4952, -0.9355, -0.6429],\n",
      "         [ 0.3763,  1.1280,  0.0219,  ..., -0.8707,  0.1372,  0.0301],\n",
      "         ...,\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [ 0.3763,  1.1280,  0.0219,  ...,  0.2047, -1.0570, -2.1961],\n",
      "         [-0.7227,  2.1221,  0.0500,  ...,  0.2047, -1.0570, -2.1961],\n",
      "         ...,\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524]],\n",
      "\n",
      "        [[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [ 0.7145,  0.8445,  0.0699,  ...,  0.2047, -1.0570, -2.1961],\n",
      "         [-0.2746,  0.4432,  0.9591,  ...,  0.2047, -1.0570, -2.1961],\n",
      "         ...,\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524]],\n",
      "\n",
      "        [[-0.4711, -1.0470,  0.4255,  ..., -0.5846,  0.7729, -0.1875],\n",
      "         [-0.5136, -0.5644, -0.9184,  ...,  0.2047, -1.0570, -2.1961],\n",
      "         [ 0.3763,  1.1280,  0.0219,  ...,  0.0507, -0.7415, -0.2291],\n",
      "         ...,\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524],\n",
      "         [-0.5692,  0.9200,  1.1108,  ..., -0.0641,  1.1783, -0.6524]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test model.get_concat_embedding\n",
    "'''\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "padded_melody, _ = pad_packed_sequence(melody, batch_first=True)\n",
    "\n",
    "concat_embedding = model.get_concat_embedding(padded_melody)\n",
    "print(f'Your concart_embedding: \\n{concat_embedding}')\n",
    "\n",
    "assert concat_embedding.shape[:-1] == padded_melody.shape[:-1], \"Num_batch and num_timestep of concat_embedding has to be the same with input melody\"\n",
    "assert concat_embedding.shape[2] == embed_size * 2, \"Error in size of embedding dimension\"\n",
    "assert (concat_embedding[0,0,:] == concat_embedding[1,0,:]).all(), \"Error: your embedding vectors for the same input notes are different\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd0050c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with ordinary tensor (using batch_size=1)\n",
    "'''\n",
    "single_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "single_batch = next(iter(single_loader))\n",
    "single_melody, single_shifted_melody = single_batch\n",
    "# print(f\"[!]single_melody: {single_melody.shape}\")\n",
    "pitch_out, dur_out = model(single_melody)\n",
    "\n",
    "assert pitch_out.shape == (1,single_melody.shape[1], model.num_pitch),  \\\n",
    "          f\"Error in pitch_out.shape. Expected {1,single_melody.shape[1], model.num_pitch}, but got {pitch_out.shape}\"\n",
    "assert dur_out.shape == (1,single_melody.shape[1], model.num_dur), \\\n",
    "          f\"Error in dur_out.shape. Expected {1,single_melody.shape[1], model.num_dur}, but got {dur_out.shape}\"\n",
    "\n",
    "assert (0<pitch_out).all() and (pitch_out<1).all() and (0<dur_out).all() and (dur_out<1).all(), \\\n",
    "          \"Every output must have a value between 0 and 1 \"\n",
    "assert (torch.abs(torch.sum(pitch_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91f6c773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pitch_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a022349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test code with PackedSequence\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "melody, shifted_melody = batch\n",
    "pitch_out, dur_out = model(melody)\n",
    "\n",
    "assert isinstance(pitch_out, type(melody)) and isinstance(dur_out, type(melody)), f\"Input of model was {type(melody)} but output is {type(pitch_out)}\"\n",
    "\n",
    "assert (pitch_out.batch_sizes == melody.batch_sizes).all(), \\\n",
    "          \"batch_sizes of input and output has to be the same\"\n",
    "assert len(pitch_out.data) == len(batch[0].data), \"Number of notes in input and output has to be the same\"\n",
    "assert (torch.abs(torch.sum(pitch_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every pitch class has to be 1\"\n",
    "assert (torch.abs(torch.sum(dur_out.data, dim=-1)-1)<1e-5).all(), \\\n",
    "          \"Sum of probability of every duration class has to be 1\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ec23",
   "metadata": {},
   "source": [
    "## Problem 6. Implement training loop (25 pts)\n",
    "- If you have succeeded in implementing model for PackedSequence, you can implement the training loop assuming that input batch is a PackedSequence\n",
    "- If not, you can implement the training loop using batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1d8d6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_distribution: \n",
      "tensor([[0.2287, 0.2227, 0.5487],\n",
      "        [0.1301, 0.4690, 0.4010],\n",
      "        [0.3269, 0.0541, 0.6190],\n",
      "        [0.0923, 0.4633, 0.4444],\n",
      "        [0.2072, 0.6336, 0.1592],\n",
      "        [0.3624, 0.2194, 0.4182],\n",
      "        [0.1773, 0.3721, 0.4505],\n",
      "        [0.6786, 0.2615, 0.0598],\n",
      "        [0.2339, 0.5722, 0.1939],\n",
      "        [0.0827, 0.1324, 0.7848]]), \n",
      " correct_class for each datasample: \n",
      " tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]])\n",
      "Loss:  tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403,\n",
      "        0.2423])\n"
     ]
    }
   ],
   "source": [
    "def get_cross_entropy_loss(prob_distribution, correct_class):\n",
    "  '''\n",
    "  This function takes predicted probability distrubtion and the corresponding correct_class.\n",
    "  \n",
    "  For example,  prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]] means that\n",
    "  for 0th data sample, the predicted probability for 0th category is 0.2287, for 1st category is 0.2227, and for 2nd category is 0.5487,\n",
    "  and for 1st data sample, the predicted probability for 0th category is 0.1301, for 1st category is 0.4690, and for 2nd category is 0.4010,\n",
    "  \n",
    "  Cross entropy, which is -y*log(y_hat), can be regarded as negative log value of predicted probability for correct class (y==1).\n",
    "  If the given correct_class is [1, 2], the loss for 0th data sample becomes negative log of [0.2287, 0.2227, 0.5487][1], which is -torch.log(0.2227), \n",
    "  because the correct category for this sample was 1st category, and the predicted probability was 0.2227\n",
    "  And the loss for 1st data sample becomes negative log of [0.1301, 0.4690, 0.4010][2], which is -torch.log(0.4010),\n",
    "  because the correct category for this sample was 2nd category, and the predicted probability was 0.4010\n",
    "  \n",
    "  To make implementation easy, let's assume we have 2D tensor for prob_distribution and  1D tensor for correct_class\n",
    "   \n",
    "  Arguments:\n",
    "    prob_distribution (2D Tensor)\n",
    "    correct_class (1D Tensor)\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Cross entropy loss for every data sample in prob_distrubition. Has a same shape with correct_class\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  Caution: When use torch.log(), don't forget to add small epsilon value (like 1e-6) to avoid infinity\n",
    "  Do not return the mean loss. Return loss that has same shape with correct_class\n",
    "  Try not to use for loop, or torch.nn.CrossEntropyLoss, or torch.nn.NLLLoss\n",
    "  \n",
    "  ì´ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ì™€ í•´ë‹¹ correct_classë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  ì˜ˆë¥¼ ë“¤ì–´ prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]]ì€ ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "  0ë²ˆì§¸ ë°ì´í„° í‘œë³¸ì˜ ê²½ìš° 0ë²ˆì§¸ ë²”ì£¼ì˜ ì˜ˆì¸¡ í™•ë¥ ì€ 0.2287, 1ë²ˆì§¸ ë²”ì£¼ì˜ ì˜ˆì¸¡ í™•ë¥ ì€ 0.227, 2ë²ˆì§¸ ë²”ì£¼ì˜ ì˜ˆì¸¡ í™•ë¥ ì€ 0.5487ìž…ë‹ˆë‹¤.\n",
    "  ì²« ë²ˆì§¸ ë°ì´í„° í‘œë³¸ì˜ ê²½ìš° 0ë²ˆì§¸ ë²”ì£¼ì˜ ì˜ˆì¸¡ í™•ë¥ ì€ 0.1301, ì²« ë²ˆì§¸ ë²”ì£¼ì˜ ê²½ìš° 0.4690, ë‘ ë²ˆì§¸ ë²”ì£¼ì˜ ê²½ìš° 0.4010ìž…ë‹ˆë‹¤.\n",
    "  \n",
    "  Cross entropy, ì¦‰ -y*log(y_hat)ëŠ” ì˜¬ë°”ë¥¸ í´ëž˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ ì˜ ìŒìˆ˜ ë¡œê·¸ ê°’(y==1)ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "  ì£¼ì–´ì§„ correct_classê°€ [1, 2]ì´ë©´ 0ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ì€ -torch.log(0.2227)ì¸ [0.2287, 0.2227, 0.5487][1]ì˜ ìŒì˜ ë¡œê·¸ê°€ ë©ë‹ˆë‹¤. \n",
    "  ì´ í‘œë³¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë²”ì£¼ëŠ” ì²« ë²ˆì§¸ ë²”ì£¼ì´ê³  ì˜ˆì¸¡ í™•ë¥ ì€ 0.227ì´ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n",
    "  ê·¸ë¦¬ê³  1ë²ˆì§¸ ë°ì´í„° í‘œë³¸ì˜ ì†ì‹¤ì€ [0.1301, 0.4690, 0.4010][2]ì˜ ìŒì˜ ë¡œê·¸ê°€ ë©ë‹ˆë‹¤. ì´ëŠ” -torch.log(0.4010)ìž…ë‹ˆë‹¤.\n",
    "  ì´ í‘œë³¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë²”ì£¼ëŠ” ë‘ ë²ˆì§¸ ë²”ì£¼ì´ê³  ì˜ˆì¸¡ í™•ë¥ ì€ 0.4010ì´ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤.\n",
    "  \n",
    "  êµ¬í˜„ì„ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ prob_distributionì— ëŒ€í•œ 2D í…ì„œì™€ correct_classì— ëŒ€í•œ 1D í…ì„œê°€ ìžˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "  '''\n",
    "  assert prob_distribution.dim() == 2 and correct_class.dim() == 1, \"Let's assume we only take 2D tensor for prob_distribution and 1D tensor for correct_class\"\n",
    "  # Write your code from here\n",
    "  cross_entropy_loss = []\n",
    "  \n",
    "  for idx in range(len(prob_distribution)):\n",
    "    loss = -torch.log(prob_distribution[idx, correct_class[idx]])\n",
    "    cross_entropy_loss.append(loss)\n",
    "  \n",
    "  return torch.stack(cross_entropy_loss)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "prob_distribution = torch.softmax(torch.randn([10, 3]), dim=-1)\n",
    "correct_class = torch.randint(0,3, [10])\n",
    "\n",
    "print(f\"prob_distribution: \\n{prob_distribution}, \\n correct_class for each datasample: \\n {correct_class.unsqueeze(1)}\")\n",
    "\n",
    "loss = get_cross_entropy_loss(prob_distribution, correct_class)\n",
    "print('Loss: ', loss)\n",
    "assert (torch.abs(loss-torch.Tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403, 0.2423]))<1e-4).all(), \"Error in loss value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "963172fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7656, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_for_single_batch(model, batch, device):\n",
    "  '''\n",
    "  This function takes model and batch and calculate Cross Entropy Loss for given batch.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    batch (batch collated by pack_collate): Tuple of (melody_batch, shifted_melody_batch)\n",
    "    device (str): cuda or cpu. In which device to calculate the batch\n",
    "    \n",
    "  Return:\n",
    "    loss (torch.Tensor): Calculated mean loss for given model and batch\n",
    "    \n",
    "  TODO: Complete this function using get_cross_entropy_loss().\n",
    "  Now you have to return the mean loss of every data sample in the batch \n",
    "  \n",
    "  Caution: You have to calculate loss for pitch, and loss for duration separately.\n",
    "  Then you can take average of pitch_loss and duration_loss\n",
    "  \n",
    "  Important Tip: If you are using PackedSequence, you can feed PackedSequence.data directly to get_cross_entropy_loss.\n",
    "  It makes the implementation much easier, because it doesn't need to reshape probabilty distribution to 2D and correct class to 1D.\n",
    "  \n",
    "  TODO: get_cross_entropy_loss()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ í•¨ìˆ˜ë¥¼ ì™„ë£Œí•©ë‹ˆë‹¤.\n",
    "  ì´ì œ ë°°ì¹˜ì— ìžˆëŠ” ëª¨ë“  ë°ì´í„° ìƒ˜í”Œì˜ í‰ê·  ì†ì‹¤ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "  \n",
    "  ì£¼ì˜: ë‹¹ì‹ ì€ pitch_lossì™€ duration_lossì„ ë”°ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  ê·¸ëŸ¬ë©´ pitch_lossì™€ duration_lossì˜ í‰ê· ì„ êµ¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "  \n",
    "  ì°¸ê³ : PackedSequenceë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° PackedSequence.dataë¥¼ get_cross_entropy_lossì— ì§ì ‘ ê³µê¸‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "  í™•ë¥  ë¶„í¬ë¥¼ 2Dë¡œ ìž¬êµ¬ì„±í•˜ê³  í´ëž˜ìŠ¤ë¥¼ 1Dë¡œ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— êµ¬í˜„ì´ í›¨ì”¬ ì‰¬ì›Œì§‘ë‹ˆë‹¤.\n",
    "  '''\n",
    "  melody_batch, shifted_melody_batch = batch\n",
    "  melody_batch = melody_batch.to(device)\n",
    "  shifted_melody_batch = shifted_melody_batch.to(device)\n",
    "\n",
    "  pred_pitch, pred_dur = model(melody_batch)\n",
    "\n",
    "  target_pitch, target_dur = shifted_melody_batch.data[..., 0], shifted_melody_batch.data[..., 1]\n",
    "  pred_pitch = pred_pitch.data.to(device)\n",
    "  pred_loss = pred_pitch.data.to(device)\n",
    "\n",
    "  pitch_loss = get_cross_entropy_loss(pred_pitch.data, target_pitch)\n",
    "  dur_loss = get_cross_entropy_loss(pred_dur.data, target_dur)\n",
    "#   return torch.stack([pitch_loss.mean(), dur_loss.mean()]).mean()\n",
    "#   pitch_loss = pitch_loss.to(device)\n",
    "#   print(pitch_loss)\n",
    "  return torch.mean((pitch_loss.float()+dur_loss.float())/2)\n",
    "\n",
    "######################################### TODO ì œì¶œ ì „ ê³ ì¹˜ê¸° #########################################\n",
    "# DEV = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# DEV = torch.device('mps:0')\n",
    "# DEV = 'mps'\n",
    "DEV = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(DEV)\n",
    "batch = next(iter(train_loader))\n",
    "get_loss_for_single_batch(model, batch, device=DEV)\n",
    "\n",
    "# model.to('cuda')\n",
    "# batch = next(iter(train_loader))\n",
    "# get_loss_for_single_batch(model, batch, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aeb02b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04243662c3446a39b076f89f216e1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0041ad9874994bad906413757eecdc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce322c7934034004a101a7167104674a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04109559462e452c8cd668660fe7db1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505e5c885d044c7dbd11028e11d268d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a0c4a923e54f7cbd8d72c788ea71b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46f88a77399415bb7dad2fc7e6106b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2ff4108c29440e8c1f2a06cf715a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1490820ff834424788ed26afc1b4ed0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7718134bae7547e7b83afb76ec380941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847927f1ffb74d4fb93604a1f3c7c5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40c81be6a65453ba512cfee1131ad67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfb107830db44cbb1766b1a1e969ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d2d05c3ca94724b53cc39a2961ef98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6f8e4ff2494b7faf2e6345743c240d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3232cf0646f4a4aace8184e9314d098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ec172a585d4365bb730edcf05e283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090c2901ace74219837dde53a8237f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e5f4b3cdb43c58629d306070fc882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3522a07a75484218b146a58bd7fcc447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fdf75c5bd444918857319b2bd025ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65282e49b234437aac7d7a6af9724e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43256b56b69e458b87c154f17ee3300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aa1cf68bef4204a490ceec612b4f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f061f9ea6e594f338c9998f8b96aa981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a789f3bc6cb645d790c0cfd1270add13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111fa5bab91b4b9da58ebe5af8a6200b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0b29d008354666a2a87326b6e37077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae70f11f5ced42a5a08c0129a0d3c85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3360cfc5319a4fe3a690d444cc41916e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1336173dd6345a88636d4effb694efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "If you have implemented the previous function correctly, this code will train the model\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "######################################### TODO: ê³ ì¹˜ê¸° #########################################\n",
    "# DEV = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "# DEV = 'mps'\n",
    "DEV = 'cuda' if torch.cuda.is_available() else 'cpu'# or cpu, but using cpu will be too slow\n",
    "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 30\n",
    "\n",
    "model.to(DEV)\n",
    "loss_record = []\n",
    "valid_loss_record = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader,leave=False):\n",
    "    loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_record.append(loss.item())\n",
    "    \n",
    "  # Validation\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    loss_for_entire_valid = 0\n",
    "    num_notes = 0\n",
    "    for batch in valid_loader:\n",
    "      loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
    "      if isinstance(batch[0], PackedSequence):\n",
    "        n_note = len(batch[0].data)        \n",
    "      else:\n",
    "        n_note = batch[0].shape[1]\n",
    "        \n",
    "      loss_for_entire_valid += loss.item() * n_note\n",
    "      num_notes += n_note\n",
    "    valid_loss_record.append(loss_for_entire_valid/num_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4be25b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x526824040>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 589,
       "width": 612
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Training Loss')\n",
    "plt.plot(loss_record)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Validation Loss')\n",
    "plt.plot(valid_loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3099763",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/MIR_HW3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ed2",
   "metadata": {},
   "source": [
    "## Problem 7: Implement Generation (25 pts)\n",
    "- In this problem, you have to generate a new melody using the trained model\n",
    "- Melody language model can generate a new sequence by sampling a new note for each timestep, and feed the generated new note again to the model to predict the next note\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd915d",
   "metadata": {},
   "source": [
    "### Problem 7-1: Implement model inference (15 pts)\n",
    "- Inference in Language model is little bit different from an ordniary forward loop during the training.\n",
    "    - While training, you have entire sequence, from beginning to end.\n",
    "    - During the inference, you have to generate one note, and then feed it as an input for the next step\n",
    "- You have to implement given functions one by one to complete `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2afa1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vec: \n",
      "tensor([[[38., 44.]],\n",
      "\n",
      "        [[38., 44.]]]) \n",
      " initial_hidden: \n",
      " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def get_initial_input_and_hidden_state(model, batch_size=1):\n",
    "  '''\n",
    "  This function generates initial input vector and hidden state for model's GRU\n",
    "  \n",
    "  To generate a new sequence, you have to provide initial seed token, which is ['start', 'start'].\n",
    "  You have to make a initial vector that has [pitch_category_index_of_'start', duration_category_index_of_'start']\n",
    "  \n",
    "  You also have to initial hidden state for the model's RNN.\n",
    "  In uni-directional RNN(or GRU), hidden state of RNN has to be a zero tensor with shape of (num_layers, batch_size, hidden_size)\n",
    "\n",
    "  \n",
    "  Argument:\n",
    "    model (MelodyLanguageModel)\n",
    "    \n",
    "  Returns:\n",
    "    initial_input_vec (torch.Tensor): Has a shape of [batch_size, 1 (timestep), 2]\n",
    "    initial_hidden (torch.Tensor): Has a shape of [num_layers, bach_size, hidden_size]\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  \n",
    "  ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ GRUì— ëŒ€í•œ ì´ˆê¸° ìž…ë ¥ ë²¡í„°ì™€ ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  \n",
    "  ìƒˆ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ë ¤ë©´ ì´ˆê¸° ì‹œë“œ í† í°(['start', 'start'])ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  ì´ˆê¸° ë²¡í„°ëŠ” [pitch_category_index_of_'start', duration_category_index_of_'start']ë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  \n",
    "  ëª¨ë¸ì˜ RNNì— ëŒ€í•œ ì´ˆê¸° ìˆ¨ê¹€ ìƒíƒœë„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "  ë‹¨ë°©í–¥ RNN(ë˜ëŠ” GRU)ì—ì„œ RNNì˜ ìˆ¨ê²¨ì§„ ìƒíƒœëŠ” (num_layers, batch_size, hidden_size)ì˜ ëª¨ì–‘ì„ ê°€ì§„ 0 í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "  '''\n",
    "  initial_token = [[model.pitch2idx['start'], model.dur2idx['start']]]\n",
    "  initial_input_vec = torch.Tensor([initial_token]*batch_size)\n",
    "\n",
    "  initial_hidden = torch.zeros((model.num_layers, batch_size, model.hidden_size))\n",
    "  return initial_input_vec, initial_hidden\n",
    "\n",
    "batch_size = 2\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=batch_size)\n",
    "print(f'input_vec: \\n{input_vec} \\n initial_hidden: \\n {initial_hidden}')\n",
    "\n",
    "assert input_vec.ndim == 3\n",
    "assert initial_hidden.ndim == 3\n",
    "assert input_vec.shape == (batch_size, 1, 2)\n",
    "assert initial_hidden.shape == (model.num_layers, batch_size, model.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c5e91",
   "metadata": {},
   "source": [
    "### Hint: Sampling from distribution\n",
    "- The language model predict probability distribution of pitch and duration for  upcoming note\n",
    "- To do that, you have to know how to sample a result from a given probability distribution\n",
    "- In PyTorch, you can use `atensor.multinomial(num_samples)`\n",
    "    - In this assignment you don't have to sample more than 1, but \n",
    "    - multinomial(num_samples=100, replacement=False) means that you want to sample 100 samples without overlapping category\n",
    "        - Thus, the total class has to be larger than 100, because you cannot sample a single category multiple time\n",
    "    - multinomial(num_samples=100, replacement=True) means that you will sample 100 from the distrubtion independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23845568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 3, 1, 2, 4, 2, 4, 4, 1, 1, 2, 4, 1, 4, 1, 1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 4948, 3: 479, 2: 2073, 4: 1490, 0: 1010})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "'''\n",
    "Example of sampling a result from a given probability distribution\n",
    "'''\n",
    "\n",
    "dummy_prob_distribution = torch.Tensor([0.1, 0.5, 0.2, 0.05, 0.15])\n",
    "sampled_out = dummy_prob_distribution.multinomial(num_samples=10000, replacement=True)\n",
    "print(sampled_out[:20])\n",
    "Counter(sampled_out.tolist()) # Number of each category sampled is almost same as num_samples * probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "15fef28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[1;32m     20\u001b[0m input_vec, initial_hidden \u001b[38;5;241m=\u001b[39m get_initial_input_and_hidden_state(model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m out_note, last_hidden \u001b[38;5;241m=\u001b[39m predict_single_step(model, input_vec, initial_hidden)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_note: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mout_note\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m last_hidden: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_hidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out_note\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def predict_single_step(model, cur_input, prev_hidden):\n",
    "  '''\n",
    "  This function runs MelodyLangaugeModel just for one step, for the given current input and previous hidden state.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_input (torch.LongTensor): Input for the current time step. Has a shape of (batch_size=1, 1 (timestep), 2)\n",
    "    prev_hidden (torch.Tensor): Hidden state of RNN after previous timestep\n",
    "\n",
    "  Returns:\n",
    "    cur_output (torch.LongTensor): Sampled note [pitch_category_idx, duration_category_idx] from the predicted probability distribution, with shape of [1,1,2]\n",
    "    last_hidden (torch.Tensor): Hidden state of RNN\n",
    "  Think about running the model.forward() step-by-step.\n",
    "  \n",
    "  input_seq â†’ self.get_concat_embedding â†’ self.rnn â†’ self.final_layer â†’ torch.softmax for [pitch, duration] â†’ sampled [pitch, duration]\n",
    "\n",
    "  '''\n",
    "  return \n",
    "\n",
    "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=1)\n",
    "out_note, last_hidden = predict_single_step(model, input_vec, initial_hidden)\n",
    "print(f'out_note: \\n{out_note} \\n last_hidden: \\n {last_hidden}')\n",
    "\n",
    "assert out_note.ndim == 3\n",
    "assert last_hidden.ndim == 3\n",
    "assert out_note.shape == (1,1,2)\n",
    "\n",
    "assert len(set([predict_single_step(model, input_vec, initial_hidden)[0] for i in range(5)]))==5, 'Generated output has to be different based on random sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_end_token(model, cur_output):\n",
    "  '''\n",
    "  During the generation, there is a possibility that the generated note predicted 'end' token for either pitch or duration.\n",
    "  (In fact, model can even estimate 'start' token during the generation even though it has very low probability)\n",
    "  \n",
    "  Using information among (model.pitch2idx, model.dur2idx, model.idx2pitch, model.idx2dur, model.num_pitch, model.num_dur), check whether \n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    cur_output (torch.LongTensor): Assume it has shape of [1,1,2 (pitch_idx, duration_idx)]\n",
    "  \n",
    "  Return:\n",
    "    is_end_token (bool): True if cur_output include category index such as 'start' or 'end',\n",
    "                          else False.\n",
    "                          \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  return \n",
    "\n",
    "\n",
    "print(is_end_token(model, out_note))\n",
    "\n",
    "assert not is_end_token(model, torch.LongTensor([[[10, 7]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[38, 40]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[25, 44]]]))\n",
    "assert is_end_token(model, torch.LongTensor([[[39, 41]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "def generate(model, random_seed=0):\n",
    "  '''\n",
    "  This function generates a new melody sequence with a given model and random_seed.\n",
    "  \n",
    "  Arguments:\n",
    "    model (MelodyLanguageModel)\n",
    "    random_seed (int): Language model's inference will always generate different result, because it uses random sampling for the prediction.\n",
    "                       Therefore, if you want to reproduce the same generation result, you have to fix random_seed.\n",
    "  \n",
    "  Returns:\n",
    "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
    "  \n",
    "  TODO: Complete this function using get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()\n",
    "  \n",
    "  Hint: You can use while loop\n",
    "        You have to track the generated single note in a list or somewhere. \n",
    "  '''\n",
    "  \n",
    "  torch.manual_seed(random_seed) # To reproduce the result, we have to control random sequence\n",
    "  \n",
    "  '''\n",
    "  Write your code from here\n",
    "  '''\n",
    "\n",
    "  return\n",
    "gen_out = generate(model)\n",
    "print(f\"gen_out: \\n {gen_out}\")\n",
    "\n",
    "assert isinstance(gen_out, torch.LongTensor), f\"output of generate() has to be torch.LongTensor, not {type(gen_out)}\"\n",
    "assert gen_out.ndim == 2, f\"output of generate() has to be 2D tensor, not {gen_out.ndim}D tensor\"\n",
    "assert gen_out.shape[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787bda",
   "metadata": {},
   "source": [
    "### Problem 7-2. Convert neural network's prediction to music score (10 pts)\n",
    "- Even though neural network has succeeded in generating a new sequence, it is just a sequence of index that neural network uses\n",
    "    - For example, generated note event [17, 10] means that this note has pitch value of 17th pitch category and duration value of 10th duration category\n",
    "- We have to convert categorical index to original value\n",
    "    - We saved this information as `idx2pitch`, `idx2dur` while we declared the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_pred_to_origin(pred:torch.Tensor, idx2pitch:list, idx2dur:list):\n",
    "  '''\n",
    "  This function convert neural net's output index to original pitch value (MIDI Pitch) and duration value \n",
    "  \n",
    "  Argument:\n",
    "    pred: generated output of the model. Has a shape of [num_notes, 2]. \n",
    "          0th dimension of each note represents pitch category index \n",
    "          and 1st dimension of each note represents duration category index\n",
    "  \n",
    "  Return:\n",
    "    converted_out (torch.Tensor): Has a same shape with 'pred'.\n",
    "    \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "    \n",
    "  return \n",
    "\n",
    "converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "assert converted_out.shape == gen_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be29831",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To solve the next problem, you have to know how note_representation looks like in muspy.\n",
    "\n",
    "In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "'''\n",
    "\n",
    "note_repr_example = train_set.dataset[0]\n",
    "note_repr_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067efdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_pitch_dur_to_note_representation(pitch_dur:torch.LongTensor):\n",
    "  '''\n",
    "  This function takes pitch_dur (shape of [num_notes, 2]) and returns the corresponding note representation (shape of [num_notes, 4])\n",
    "  In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
    "  \n",
    "  Since our generation is monophonic, you can regard start_timestep starts from 0 and accumulate the duration of note.\n",
    "  You can fix velocity to 64.\n",
    "  \n",
    "  \n",
    "  Arguments:\n",
    "    pitch_dur: LongTensor of note where each note represented as pitch and duration value\n",
    "    \n",
    "  return:\n",
    "    note_repr: numpy.Array with shape of [num_notes, 4]\n",
    "               each note has value of [start_timestep, pitch, duration, velocity]\n",
    "\n",
    "  TODO: Complete this function\n",
    "  Hint: You can use torch.cumsum() to accumulate the duration.\n",
    "  To convert torch tensor to numpy, you can use atensor.numpy()\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  return \n",
    "\n",
    "note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "note_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d9303",
   "metadata": {},
   "source": [
    "## Generation: Visualize and synthesize the generated result (10 pts)\n",
    "- Try to generate different melody using different `random_seed`\n",
    "- In your submission, include **Three** examples of your favorite among the generated results in wav\n",
    "    - You have to install soundfont and music font using \n",
    "        - `muspy.download_bravura_font()`\n",
    "        - `muspy.download_musescore_soundfont()`\n",
    "    - You may need fluidsynth to synthesize the sound.\n",
    "        - In colab, `!sudo apt-get install fluidsynth` will work\n",
    "        - In other Ubuntu os, `sudo apt-get update` and then `sudo apt-get install fluidsynth` will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71a3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "gen_music = muspy.from_note_representation(note_repr)\n",
    "gen_music.show_score()\n",
    "\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9a3cb",
   "metadata": {},
   "source": [
    "- Try with different random seed and generate interesting melodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_muspy_music(model, random_seed=0):\n",
    "  '''\n",
    "  This function combines 'generate', 'convert_idx_pred_to_origin', 'convert_pitch_dur_to_note_representation', muspy.from_note_representation\n",
    "  '''\n",
    "  gen_out = generate(model, random_seed)\n",
    "  converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
    "  note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
    "  gen_music = muspy.from_note_representation(note_repr)\n",
    "  return gen_music\n",
    "\n",
    "gen_music = generate_muspy_music(model, random_seed=2)\n",
    "gen_music.show_score()\n",
    "gen_audio = gen_music.synthesize().T\n",
    "ipd.Audio(gen_audio/2**15, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed38eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can save audio as wave file with muspy.write_audio\n",
    "'''\n",
    "gen_music.write_audio('result_0.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
